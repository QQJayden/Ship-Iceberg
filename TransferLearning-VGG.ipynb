{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransferLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 参考kernel:[Transfer Learning with VGG-16 ConvNet LB 0.1850](https://www.kaggle.com/devm2024/transfer-learning-with-vgg-16-convnet-lb-0-1850)\n",
    "+ 参考kernel:[Transfer Learning with VGG-16 CNN+AUG LB 0.1712](https://www.kaggle.com/devm2024/transfer-learning-with-vgg-16-cnn-aug-lb-0-1712)\n",
    "+ 参考文章：[VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](https://arxiv.org/pdf/1409.1556.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改进要点：\n",
    "1. 改进第三通道？？\n",
    "2. 模型优化？？\n",
    "3. 多结果集成？\n",
    "4. 第二次训练 显存占用率降不下来，提示Memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mandatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"../ShipIceberg/Data/train.json\")\n",
    "target_train=train['is_iceberg']\n",
    "\n",
    "# 一直cannot resolve memory block\n",
    "# test = pd.read_json(\"../ShipIceberg/Data/test.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../ShipIceberg/Data/test.json', 'r') as f:\n",
    "    test = json.load(f)\n",
    "    test=pd.DataFrame(test)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train=train['is_iceberg']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "X_angle=train['inc_angle']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "X_test_angle=test['inc_angle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Isolation function.\n",
    "def iso(arr):\n",
    "    p = np.reshape(np.array(arr), [75,75]) >(np.mean(np.array(arr))+2*np.std(np.array(arr)))\n",
    "    return p * np.reshape(np.array(arr), [75,75])\n",
    "\n",
    "# Size in number of pixels of every isolated object.\n",
    "def size(arr):     \n",
    "    return np.sum(arr<-5)/(75*75)\n",
    "\n",
    "# Feature engineering iso1 and iso2.\n",
    "train['iso1'] = train.iloc[:, 0].apply(iso)\n",
    "train['iso2'] = train.iloc[:, 1].apply(iso)\n",
    "\n",
    "# train.head(5)\n",
    "\n",
    "\n",
    "# Feature engineering s1 s2 and size.\n",
    "train['s1'] = train.iloc[:,5].apply(size)\n",
    "train['s2'] = train.iloc[:,6].apply(size)\n",
    "train['size'] = train.s1+train.s2\n",
    "\n",
    "X_size = train['size']\n",
    "\n",
    "# Feature engineering iso1 and iso2.\n",
    "test['iso1'] = test.iloc[:, 0].apply(iso)\n",
    "test['iso2'] = test.iloc[:, 1].apply(iso)\n",
    "\n",
    "# test.head(5)\n",
    "\n",
    "test['s1'] = test.iloc[:,4].apply(size)\n",
    "test['s2'] = test.iloc[:,5].apply(size)\n",
    "test['size'] = test.s1+test.s2\n",
    "\n",
    "\n",
    "X_test_size = test['size']\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "        # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "        # window: descpeckling filter window (tuple)\n",
    "        # default noise variance = 0.25\n",
    "        # assumes noise mean = 0\n",
    "    \n",
    "        mean_window = uniform_filter(band, window)\n",
    "        mean_sqr_window = uniform_filter(band**2, window)\n",
    "        var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "        weights = var_window / (var_window + var_noise)\n",
    "        band_filtered = mean_window + weights*(band - mean_window)\n",
    "        return band_filtered\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_images(df):\n",
    "    '''Create 3-channel 'images'. Return rescale-normalised images.'''\n",
    "    images = []\n",
    "    for i, row in df.iterrows():\n",
    "        # Formulate the bands as 75x75 arrays\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        \n",
    "        #band_3 = band_1 / band_2\n",
    "        band_3 = (band_1 + band_2)/2\n",
    "\n",
    "        # Rescale2\n",
    "        #r = ((band_1 - np.mean(band_1)) / (np.max(band_1) - np.min(band_1)))\n",
    "        #g = ((band_2 - np.mean(band_2)) / (np.max(band_2) - np.min(band_2)))\n",
    "        #b = ((band_3 - np.mean(band_3)) / (np.max(band_3) - np.min(band_3)))\n",
    "        #r = (band_1 - band_1.min()) / (band_1.max() - band_1.min())\n",
    "        #g = (band_2 - band_2.min()) / (band_2.max() - band_2.min())\n",
    "        #b = (band_3 - band_3.min()) / (band_3.max() - band_3.min())\n",
    "        r = (band_1 + abs(band_1.min())) / np.max((band_1 + abs(band_1.min())))\n",
    "        g = (band_2 + abs(band_2.min())) / np.max((band_2 + abs(band_2.min())))\n",
    "        b = (band_3 + abs(band_3.min())) / np.max((band_3 + abs(band_3.min())))\n",
    "\n",
    "        rgb = np.dstack((r, g, b))\n",
    "        images.append(rgb)\n",
    "    return np.array(images)\n",
    "\n",
    "X_train = get_images(train)\n",
    "X_test = get_images(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "\n",
    "'''\n",
    "# 需要循环？？？\n",
    "# let's see if despeckling has any influence on the images\n",
    "# plot nine different instances with different windows and noise levels (band 1)\n",
    "windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "noise_var = np.array([1, 2, 4])\n",
    "\n",
    "# 0-2\n",
    "Wi = 2\n",
    "Nj = 2\n",
    "\n",
    "for i in range(0,1604):\n",
    "    band_1_linear = decibel_to_linear(X_band_1[i])\n",
    "    band_2_linear = decibel_to_linear(X_band_2[i])\n",
    "\n",
    "    noise_var_1 = np.round(np.var(band_1_linear)*noise_var,10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear)*noise_var,10)\n",
    "\n",
    "    X_band_1[i] = linear_to_decibel(lee_filter(band_1_linear, windows[Wi], noise_var_1[Nj]))\n",
    "    X_band_2[i] = linear_to_decibel(lee_filter(band_2_linear, windows[Wi], noise_var_2[Nj]))\n",
    "\n",
    "\n",
    "# Sobel Operator & Averaging\n",
    "from scipy import signal\n",
    "\n",
    "Gx_sobel=np.array([[1,0,-1],[2,0,-2],[1,0,-1]])\n",
    "Gy_sobel=np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "lpf=1.0/25*np.ones((5,5))\n",
    "\n",
    "for i in range(0,1604):\n",
    "    \n",
    "    img_1=X_band_1[i].copy()\n",
    "    img_lpf_1=signal.convolve2d(img_1,lpf,mode='same')\n",
    "    img_Gx_1=signal.convolve2d(img_lpf_1,Gx_sobel,mode='same')\n",
    "    img_Gy_1=signal.convolve2d(img_lpf_1,Gy_sobel,mode='same')\n",
    "\n",
    "    img_2=X_band_2[i].copy()\n",
    "    img_lpf_2=signal.convolve2d(img_2,lpf,mode='same')\n",
    "    img_Gx_2=signal.convolve2d(img_lpf_2,Gx_sobel,mode='same')\n",
    "    img_Gy_2=signal.convolve2d(img_lpf_2,Gy_sobel,mode='same')\n",
    "\n",
    "    X_band_1[i] = np.hypot(img_Gx_1,img_Gy_1)\n",
    "    X_band_2[i] = np.hypot(img_Gx_2,img_Gy_2)\n",
    "'''\n",
    "\n",
    "X_band_3 = np.zeros((1604,75,75))\n",
    "for i in range(0,1604):\n",
    "    subt = abs(X_band_1[i]-X_band_2[i])\n",
    "    W1 = subt/subt.max()\n",
    "    W2=1-W1\n",
    "    X_band_3[i]=W1 * X_band_1[i]+W2 * X_band_2[i]\n",
    "\n",
    "\n",
    "# X_band_3=(X_band_1+X_band_2)/2  # 算术平均：尝试其他比例？\n",
    "#X_band_3=X_band_1 * 0.5858 + X_band_2 * 0.4142\n",
    "# 效果极差 X_band_3 = X_band_2 * X_band_2 / X_band_1\n",
    "#X_band_3 = np.sqrt(X_band_1 * X_band_2)  # 调和·平均: sqrt invalid number???\n",
    "#X_band_3 = X_band_1 / X_band_2\n",
    "# X_band_3=X_band_2 * 2 - X_band_1\n",
    "\n",
    "#from scipy import signal\n",
    "#X_band_3=signal.fftconvolve(X_band_1, X_band_2, mode = 'same')\n",
    "\n",
    "#X_band_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in train[\"inc_angle\"]])\n",
    "\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n",
    "                          , X_band_2[:, :, :, np.newaxis]\n",
    "                         , X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "\n",
    "#Generate the test data\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "\n",
    "'''\n",
    "for i in range(0,8424):\n",
    "    band_test_1_linear = decibel_to_linear(X_band_test_1[i])\n",
    "    band_test_2_linear = decibel_to_linear(X_band_test_2[i])\n",
    "\n",
    "    noise_var_test_1 = np.round(np.var(band_test_1_linear)*noise_var,10)\n",
    "    noise_var_test_2 = np.round(np.var(band_test_2_linear)*noise_var,10)\n",
    "\n",
    "    X_band_test_1[i] = linear_to_decibel(lee_filter(band_test_1_linear, windows[Wi], noise_var_test_1[Nj]))\n",
    "    X_band_test_2[i] = linear_to_decibel(lee_filter(band_test_2_linear, windows[Wi], noise_var_test_2[Nj]))\n",
    "\n",
    "\n",
    "for i in range(0,8424):\n",
    "    \n",
    "    img_1=X_band_test_1[i].copy()\n",
    "    img_lpf_1=signal.convolve2d(img_1,lpf,mode='same')\n",
    "    img_Gx_1=signal.convolve2d(img_lpf_1,Gx_sobel,mode='same')\n",
    "    img_Gy_1=signal.convolve2d(img_lpf_1,Gy_sobel,mode='same')\n",
    "\n",
    "    img_2=X_band_test_2[i].copy()\n",
    "    img_lpf_2=signal.convolve2d(img_2,lpf,mode='same')\n",
    "    img_Gx_2=signal.convolve2d(img_lpf_2,Gx_sobel,mode='same')\n",
    "    img_Gy_2=signal.convolve2d(img_lpf_2,Gy_sobel,mode='same')\n",
    "\n",
    "    X_band_test_1[i] = np.hypot(img_Gx_1,img_Gy_1)\n",
    "    X_band_test_2[i] = np.hypot(img_Gx_2,img_Gy_2)\n",
    "'''\n",
    "\n",
    "X_band_test_3 = np.zeros((8424,75,75))\n",
    "for i in range(0,8424):\n",
    "    subt = abs(X_band_test_1[i]-X_band_test_2[i])\n",
    "    W1 = subt/subt.max()\n",
    "    W2=1-W1\n",
    "    X_band_test_3[i]=W1 * X_band_test_1[i]+W2 * X_band_test_2[i]\n",
    "\n",
    "# X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "#X_band_test_3=X_band_test_1 * 0.5858 + X_band_test_2 * 0.4142\n",
    "#X_band_test_3 = X_band_test_2 * X_band_test_2 / X_band_test_1\n",
    "#X_band_test_3 = np.sqrt(X_band_test_1*X_band_test_2)\n",
    "#X_band_test_3=signal.fftconvolve(X_band_test_1, X_band_test_2, mode = 'same')\n",
    "#X_band_test_3 = X_band_test_1 / X_band_test_2\n",
    "# X_band_test_3=X_band_test_2 * 2 - X_band_test_1\n",
    "\n",
    "#X_band_test_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in test[\"inc_angle\"]])\n",
    "\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , X_band_test_3[:, :, :, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Keras.\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "#from keras.optimizers import rmsprop\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "#from keras.applications.vgg19 import preprocess_input\n",
    "#from keras.applications.xception import preprocess_input\n",
    "#from keras.applications.inception_v3 import preprocess_input\n",
    "# ResNet\n",
    "\n",
    "#Data Aug for multi-input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 批处理大小的设置\n",
    "# batch_size=64\n",
    "batch_size=64\n",
    "\n",
    "# Define the image transformations here\n",
    "# 原始旋转范围10\n",
    "#gen = ImageDataGenerator(horizontal_flip = True,\n",
    "#                         vertical_flip = True,\n",
    "#                         width_shift_range = 0.,\n",
    "#                         height_shift_range = 0.,\n",
    "#                         channel_shift_range=0,\n",
    "#                         zoom_range = 0.2,\n",
    "#                         rotation_range = 10)\n",
    "\n",
    "\n",
    "\n",
    "# aug1\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.,\n",
    "                         height_shift_range = 0.,\n",
    "                         channel_shift_range=0.,\n",
    "                         zoom_range = 0.,\n",
    "                         rotation_range = 0)\n",
    "\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_two_inputs(X1, X2, X3, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n",
    "    genX3 = gen.flow(X1,X3, batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            X3i = genX3.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1], X3i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "def get_callbacks(filepath, patience=2):\n",
    "   es = EarlyStopping('val_loss', patience=10, mode=\"min\")\n",
    "   msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "   return [es, msave]\n",
    "\n",
    "\n",
    "def getVggAngleModel():\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "    angle_layer = Dense(1, )(input_2)\n",
    "    \n",
    "    input_3 = Input(shape=[1], name=\"size\")\n",
    "    size_layer = Dense(1, )(input_3)\n",
    "    \n",
    "    # VGG16换成其他模型？？\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, \n",
    "                 input_shape=X_train.shape[1:], classes=1)\n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "  \n",
    "    '''\n",
    "    # BatchNorm 未形成conv bn scale relu 的block\n",
    "    input_tensor = Input(shape=X_train.shape[1:])\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "#     x = base_model(bn)\n",
    "    x = base_model.get_layer('block5_pool')(bn)\n",
    "'''\n",
    "#     x = GlobalMaxPooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    merge_one = concatenate([x, angle_layer,size_layer])\n",
    "#     merge_one = x\n",
    "    merge_one = Dense(512, activation='relu', name='fc2')(merge_one)\n",
    "#     merge_one = Dense(1024, activation='relu', name='fc2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=[base_model.input, input_2,input_3], output=predictions)\n",
    "#     model = Model(input=[base_model.input], output=predictions)\n",
    "#     model = Model(input=[input_tensor, input_2], output=predictions)\n",
    "    \n",
    "    # 使用不同的优化\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adagrad = Adagrad(lr = 1e-3, epsilon = 1e-6)\n",
    "    rmsprop = RMSprop(lr=1e-3, rho = 0.9, epsilon=1e-6)\n",
    "    adadelta = Adadelta(lr=1e-3, rho=0.95, epsilon=1e-06)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    adamax = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    nadam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "    \n",
    "    # 更换loss\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Using K-fold Cross Validation with Data Augmentation.\n",
    "def myAngleCV(X_train, X_angle, X_size, X_test):\n",
    "    # K-折交叉验证\n",
    "    K=4\n",
    "    \n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    \n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "        #Angle\n",
    "        X_angle_cv=X_angle[train_idx]\n",
    "        X_angle_hold=X_angle[test_idx]\n",
    "        \n",
    "        X_size_cv=X_size[train_idx]\n",
    "        X_size_hold=X_size[test_idx]\n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"./model/%s_aug_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv,X_size_cv, y_train_cv)\n",
    "        galaxyModel= getVggAngleModel()\n",
    "        \n",
    "        # 调整训练参数\n",
    "        galaxyModel.fit_generator(\n",
    "                gen_flow,\n",
    "#                 steps_per_epoch=24,\n",
    "                steps_per_epoch=len(X_train_cv)//batch_size,\n",
    "                #steps_per_epoch=100,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=([X_holdout,X_angle_hold,X_size_hold], Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        galaxyModel.load_weights(filepath=file_path)\n",
    "        #Getting Training Score\n",
    "        score = galaxyModel.evaluate([X_train_cv,X_angle_cv, X_size_cv], y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        #Getting Test Score\n",
    "        score = galaxyModel.evaluate([X_holdout,X_angle_hold, X_size_hold], Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.\n",
    "        pred_valid=galaxyModel.predict([X_holdout,X_angle_hold, X_size_hold])\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "        temp_test=galaxyModel.predict([X_test, X_test_angle, X_test_size])\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train Scores\n",
    "        temp_train=galaxyModel.predict([X_train, X_angle, X_size])\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation= ',log_loss(target_train, y_train_pred_log))\n",
    "    print(' Test Log Loss Validation= ',log_loss(target_train, y_valid_pred_log))\n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayden/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:120: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 19s - loss: 0.6591 - acc: 0.6033 - val_loss: 0.4187 - val_acc: 0.7935\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 10s - loss: 0.5005 - acc: 0.7373 - val_loss: 0.4131 - val_acc: 0.7587\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 8s - loss: 0.3482 - acc: 0.8242 - val_loss: 0.4746 - val_acc: 0.7761\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 9s - loss: 0.3219 - acc: 0.8681 - val_loss: 0.2475 - val_acc: 0.8881\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 9s - loss: 0.2611 - acc: 0.8855 - val_loss: 0.2338 - val_acc: 0.8806\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 9s - loss: 0.2131 - acc: 0.9120 - val_loss: 0.2224 - val_acc: 0.9030\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 9s - loss: 0.2443 - acc: 0.9057 - val_loss: 0.2106 - val_acc: 0.9030\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 8s - loss: 0.2224 - acc: 0.9092 - val_loss: 0.2066 - val_acc: 0.9080\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 8s - loss: 0.1838 - acc: 0.9313 - val_loss: 0.2085 - val_acc: 0.9154\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 8s - loss: 0.1882 - acc: 0.9251 - val_loss: 0.2262 - val_acc: 0.9129\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 8s - loss: 0.1703 - acc: 0.9283 - val_loss: 0.2283 - val_acc: 0.9104\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 8s - loss: 0.1710 - acc: 0.9244 - val_loss: 0.2330 - val_acc: 0.9080\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 8s - loss: 0.1549 - acc: 0.9298 - val_loss: 0.2318 - val_acc: 0.9104\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 8s - loss: 0.1341 - acc: 0.9422 - val_loss: 0.1982 - val_acc: 0.9303\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 8s - loss: 0.1690 - acc: 0.9344 - val_loss: 0.2665 - val_acc: 0.9005\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 8s - loss: 0.1321 - acc: 0.9566 - val_loss: 0.2327 - val_acc: 0.9179\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 8s - loss: 0.1159 - acc: 0.9583 - val_loss: 0.2221 - val_acc: 0.9154\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 8s - loss: 0.1190 - acc: 0.9570 - val_loss: 0.2385 - val_acc: 0.9104\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 8s - loss: 0.1116 - acc: 0.9607 - val_loss: 0.2028 - val_acc: 0.9204\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 8s - loss: 0.0943 - acc: 0.9688 - val_loss: 0.2750 - val_acc: 0.9229\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 8s - loss: 0.0955 - acc: 0.9616 - val_loss: 0.2747 - val_acc: 0.9179\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 8s - loss: 0.0997 - acc: 0.9537 - val_loss: 0.2364 - val_acc: 0.9179\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 8s - loss: 0.0859 - acc: 0.9704 - val_loss: 0.2431 - val_acc: 0.9129\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 8s - loss: 0.0808 - acc: 0.9658 - val_loss: 0.2694 - val_acc: 0.9179\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 8s - loss: 0.0810 - acc: 0.9694 - val_loss: 0.2850 - val_acc: 0.9154\n",
      "Train loss: 0.109636091911\n",
      "Train accuracy: 0.961730449747\n",
      "Test loss: 0.198243622756\n",
      "Test accuracy: 0.930348259003\n",
      "\n",
      "===================FOLD= 1\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 10s - loss: 0.8528 - acc: 0.5373 - val_loss: 0.5047 - val_acc: 0.7656\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 10s - loss: 0.6495 - acc: 0.6580 - val_loss: 0.4345 - val_acc: 0.8030\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 9s - loss: 0.4697 - acc: 0.7836 - val_loss: 0.2703 - val_acc: 0.8928\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 9s - loss: 0.3651 - acc: 0.8353 - val_loss: 0.2171 - val_acc: 0.9352\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 8s - loss: 0.3173 - acc: 0.8580 - val_loss: 0.2291 - val_acc: 0.9027\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 9s - loss: 0.2730 - acc: 0.8797 - val_loss: 0.1847 - val_acc: 0.9327\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 9s - loss: 0.2404 - acc: 0.9015 - val_loss: 0.1838 - val_acc: 0.9426\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 8s - loss: 0.2251 - acc: 0.9010 - val_loss: 0.1869 - val_acc: 0.9227\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 9s - loss: 0.2231 - acc: 0.9145 - val_loss: 0.1640 - val_acc: 0.9451\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 8s - loss: 0.1948 - acc: 0.9210 - val_loss: 0.1791 - val_acc: 0.9401\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 9s - loss: 0.1994 - acc: 0.9221 - val_loss: 0.1585 - val_acc: 0.9401\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 8s - loss: 0.2014 - acc: 0.9193 - val_loss: 0.1670 - val_acc: 0.9401\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 8s - loss: 0.1583 - acc: 0.9360 - val_loss: 0.1795 - val_acc: 0.9426\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 8s - loss: 0.1880 - acc: 0.9243 - val_loss: 0.1856 - val_acc: 0.9302\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 8s - loss: 0.1459 - acc: 0.9442 - val_loss: 0.1750 - val_acc: 0.9426\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 8s - loss: 0.1330 - acc: 0.9488 - val_loss: 0.2101 - val_acc: 0.9327\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 8s - loss: 0.1567 - acc: 0.9416 - val_loss: 0.1658 - val_acc: 0.9426\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 8s - loss: 0.1462 - acc: 0.9375 - val_loss: 0.2003 - val_acc: 0.9277\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 9s - loss: 0.1439 - acc: 0.9340 - val_loss: 0.2237 - val_acc: 0.9152\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 8s - loss: 0.1247 - acc: 0.9540 - val_loss: 0.1876 - val_acc: 0.9327\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 8s - loss: 0.1404 - acc: 0.9460 - val_loss: 0.1842 - val_acc: 0.9302\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 8s - loss: 0.1053 - acc: 0.9607 - val_loss: 0.2435 - val_acc: 0.9052\n",
      "Train loss: 0.158143795352\n",
      "Train accuracy: 0.940149626629\n",
      "Test loss: 0.158520678778\n",
      "Test accuracy: 0.940149625935\n",
      "\n",
      "===================FOLD= 2\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 9s - loss: 0.8824 - acc: 0.5599 - val_loss: 0.8288 - val_acc: 0.5387\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 8s - loss: 0.6103 - acc: 0.6827 - val_loss: 0.4262 - val_acc: 0.8080\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 8s - loss: 0.4095 - acc: 0.8120 - val_loss: 0.3437 - val_acc: 0.8354\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 8s - loss: 0.3274 - acc: 0.8429 - val_loss: 0.3225 - val_acc: 0.8379\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 8s - loss: 0.2794 - acc: 0.8747 - val_loss: 0.3290 - val_acc: 0.8429\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 8s - loss: 0.2447 - acc: 0.8830 - val_loss: 0.3411 - val_acc: 0.8653\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 8s - loss: 0.2569 - acc: 0.8847 - val_loss: 0.2598 - val_acc: 0.8903\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 8s - loss: 0.2287 - acc: 0.9017 - val_loss: 0.2852 - val_acc: 0.8803\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 8s - loss: 0.2109 - acc: 0.9121 - val_loss: 0.2534 - val_acc: 0.8953\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 8s - loss: 0.2279 - acc: 0.9021 - val_loss: 0.2895 - val_acc: 0.8753\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 8s - loss: 0.1831 - acc: 0.9277 - val_loss: 0.2819 - val_acc: 0.8903\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 8s - loss: 0.2252 - acc: 0.8995 - val_loss: 0.2274 - val_acc: 0.9127\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 8s - loss: 0.1537 - acc: 0.9366 - val_loss: 0.2596 - val_acc: 0.8828\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 8s - loss: 0.1646 - acc: 0.9358 - val_loss: 0.2726 - val_acc: 0.8928\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 8s - loss: 0.1653 - acc: 0.9379 - val_loss: 0.2454 - val_acc: 0.8978\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 8s - loss: 0.1715 - acc: 0.9310 - val_loss: 0.2251 - val_acc: 0.9127\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 8s - loss: 0.1394 - acc: 0.9462 - val_loss: 0.2264 - val_acc: 0.9152\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 8s - loss: 0.1573 - acc: 0.9308 - val_loss: 0.2604 - val_acc: 0.8953\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 8s - loss: 0.1349 - acc: 0.9477 - val_loss: 0.2489 - val_acc: 0.8953\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 8s - loss: 0.1416 - acc: 0.9418 - val_loss: 0.2541 - val_acc: 0.9002\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 8s - loss: 0.1017 - acc: 0.9627 - val_loss: 0.3018 - val_acc: 0.9052\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 8s - loss: 0.1194 - acc: 0.9497 - val_loss: 0.2669 - val_acc: 0.9152\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 8s - loss: 0.1146 - acc: 0.9581 - val_loss: 0.2869 - val_acc: 0.9102\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 8s - loss: 0.1147 - acc: 0.9538 - val_loss: 0.2526 - val_acc: 0.9127\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 8s - loss: 0.1111 - acc: 0.9535 - val_loss: 0.2881 - val_acc: 0.9077\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 8s - loss: 0.0904 - acc: 0.9648 - val_loss: 0.2969 - val_acc: 0.9027\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 8s - loss: 0.0942 - acc: 0.9703 - val_loss: 0.3115 - val_acc: 0.9102\n",
      "Train loss: 0.137557884937\n",
      "Train accuracy: 0.949293433084\n",
      "Test loss: 0.225120061688\n",
      "Test accuracy: 0.912718207164\n",
      "\n",
      "===================FOLD= 3\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 10s - loss: 0.6773 - acc: 0.6146 - val_loss: 0.3978 - val_acc: 0.7925\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 10s - loss: 0.4576 - acc: 0.7635 - val_loss: 0.3280 - val_acc: 0.8325\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 8s - loss: 0.3026 - acc: 0.8581 - val_loss: 0.3291 - val_acc: 0.8625\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 8s - loss: 0.2629 - acc: 0.8863 - val_loss: 0.3863 - val_acc: 0.8200\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 8s - loss: 0.2471 - acc: 0.9009 - val_loss: 0.2567 - val_acc: 0.8850\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 8s - loss: 0.2337 - acc: 0.9116 - val_loss: 0.2903 - val_acc: 0.8800\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 8s - loss: 0.2294 - acc: 0.9061 - val_loss: 0.2933 - val_acc: 0.8600\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 8s - loss: 0.1789 - acc: 0.9349 - val_loss: 0.2515 - val_acc: 0.8925\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 8s - loss: 0.1522 - acc: 0.9378 - val_loss: 0.2775 - val_acc: 0.8925\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 8s - loss: 0.1912 - acc: 0.9287 - val_loss: 0.3077 - val_acc: 0.8650\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 8s - loss: 0.1448 - acc: 0.9428 - val_loss: 0.2646 - val_acc: 0.8925\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 8s - loss: 0.1471 - acc: 0.9453 - val_loss: 0.2604 - val_acc: 0.8900\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 8s - loss: 0.1466 - acc: 0.9417 - val_loss: 0.2684 - val_acc: 0.8900\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 8s - loss: 0.1209 - acc: 0.9544 - val_loss: 0.2742 - val_acc: 0.9000\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 8s - loss: 0.1475 - acc: 0.9400 - val_loss: 0.2685 - val_acc: 0.8925\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 8s - loss: 0.1104 - acc: 0.9512 - val_loss: 0.2455 - val_acc: 0.9075\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 8s - loss: 0.0989 - acc: 0.9631 - val_loss: 0.2674 - val_acc: 0.8950\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 8s - loss: 0.1012 - acc: 0.9588 - val_loss: 0.3522 - val_acc: 0.8875\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 8s - loss: 0.1012 - acc: 0.9593 - val_loss: 0.3100 - val_acc: 0.8875\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 8s - loss: 0.0868 - acc: 0.9531 - val_loss: 0.3341 - val_acc: 0.8875\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 8s - loss: 0.1107 - acc: 0.9610 - val_loss: 0.2907 - val_acc: 0.8975\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 8s - loss: 0.1030 - acc: 0.9590 - val_loss: 0.2822 - val_acc: 0.8950\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 8s - loss: 0.0810 - acc: 0.9729 - val_loss: 0.3020 - val_acc: 0.8800\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 8s - loss: 0.0898 - acc: 0.9607 - val_loss: 0.3316 - val_acc: 0.8875\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 8s - loss: 0.0754 - acc: 0.9807 - val_loss: 0.3387 - val_acc: 0.8900\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 8s - loss: 0.0770 - acc: 0.9675 - val_loss: 0.3751 - val_acc: 0.8875\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 8s - loss: 0.0742 - acc: 0.9664 - val_loss: 0.2850 - val_acc: 0.9125\n",
      "Train loss: 0.0770843518632\n",
      "Train accuracy: 0.97508305628\n",
      "Test loss: 0.245516327918\n",
      "Test accuracy: 0.9075\n",
      "\n",
      " Train Log Loss Validation=  0.122274508519\n",
      " Test Log Loss Validation=  0.206820700941\n"
     ]
    }
   ],
   "source": [
    "preds=myAngleCV(X_train, X_angle, X_size, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Submission for each day.\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['is_iceberg']=preds\n",
    "submission.to_csv('./submission/subVGG7.18.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
