{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning VGG16 - 改 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "+ [Exploration & Transforming Images in Python](https://www.kaggle.com/muonneutrino/exploration-transforming-images-in-python)\n",
    "+ [Transfer Learning with VGG-16 CNN+AUG LB 0.1712](http://localhost:8888/notebooks/kaggle/ShipIceberg/TransferLearning-VGG.ipynb)\n",
    "+ [Despeckling Synthetic Aperture Radar (SAR) Images](https://www.kaggle.com/jgroff/despeckling-synthetic-aperture-radar-sar-images)\n",
    "+ [Submarineering. Size Matters](https://www.kaggle.com/submarineering/submarineering-size-matters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mandatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = pd.read_json(\"../ShipIceberg/Data/train_model_fill.json\")\n",
    "train = pd.read_json(\"../ShipIceberg/Data/train.json\")\n",
    "target_train=train['is_iceberg']\n",
    "\n",
    "# 一直cannot resolve memory block\n",
    "# test = pd.read_json(\"../ShipIceberg/Data/test.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../ShipIceberg/Data/test.json', 'r') as f:\n",
    "    test = json.load(f)\n",
    "    test=pd.DataFrame(test)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train['inc_angle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train=train['is_iceberg']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "X_angle=train['inc_angle']\n",
    "# test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "# train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "X_test_angle=test['inc_angle']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "        # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "        # window: descpeckling filter window (tuple)\n",
    "        # default noise variance = 0.25\n",
    "        # assumes noise mean = 0\n",
    "    \n",
    "        mean_window = uniform_filter(band, window)\n",
    "        mean_sqr_window = uniform_filter(band**2, window)\n",
    "        var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "        weights = var_window / (var_window + var_noise)\n",
    "        band_filtered = mean_window + weights*(band - mean_window)\n",
    "        return band_filtered\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Isolation function.\n",
    "def iso(arr):\n",
    "    p = np.reshape(np.array(arr), [75,75]) >(np.mean(np.array(arr))+2*np.std(np.array(arr)))\n",
    "    return p * np.reshape(np.array(arr), [75,75])\n",
    "\n",
    "# Size in number of pixels of every isolated object.\n",
    "def size(arr):     \n",
    "    return np.sum(arr<-5)\n",
    "\n",
    "# Feature engineering iso1 and iso2.\n",
    "train['iso1'] = train.iloc[:, 0].apply(iso)\n",
    "train['iso2'] = train.iloc[:, 1].apply(iso)\n",
    "\n",
    "# train.head(5)\n",
    "\n",
    "'''\n",
    "# Feature engineering s1 s2 and size.\n",
    "train['s1'] = train.iloc[:,5].apply(size)\n",
    "train['s2'] = train.iloc[:,6].apply(size)\n",
    "train['size'] = train.s1+train.s2\n",
    "\n",
    "X_size = train['size']\n",
    "\n",
    "# Feature engineering iso1 and iso2.\n",
    "test['iso1'] = test.iloc[:, 0].apply(iso)\n",
    "test['iso2'] = test.iloc[:, 1].apply(iso)\n",
    "\n",
    "# test.head(5)\n",
    "\n",
    "test['s1'] = test.iloc[:,4].apply(size)\n",
    "test['s2'] = test.iloc[:,5].apply(size)\n",
    "test['size'] = test.s1+test.s2\n",
    "\n",
    "\n",
    "X_test_size = test['size']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "\n",
    "'''\n",
    "# 需要循环？？？\n",
    "# let's see if despeckling has any influence on the images\n",
    "# plot nine different instances with different windows and noise levels (band 1)\n",
    "windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "noise_var = np.array([1, 2, 4])\n",
    "\n",
    "# 0-2\n",
    "Wi = 0\n",
    "Nj = 0\n",
    "\n",
    "for i in range(0,1604):\n",
    "    band_1_linear = decibel_to_linear(X_band_1[i])\n",
    "    band_2_linear = decibel_to_linear(X_band_2[i])\n",
    "\n",
    "    noise_var_1 = np.round(np.var(band_1_linear)*noise_var,10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear)*noise_var,10)\n",
    "\n",
    "    X_band_1[i] = linear_to_decibel(lee_filter(band_1_linear, windows[Wi], noise_var_1[Nj]))\n",
    "    X_band_2[i] = linear_to_decibel(lee_filter(band_2_linear, windows[Wi], noise_var_2[Nj]))\n",
    "\n",
    "\n",
    "# Sobel Operator & Averaging\n",
    "from scipy import signal\n",
    "\n",
    "Gx_sobel=np.array([[1,0,-1],[2,0,-2],[1,0,-1]])\n",
    "Gy_sobel=np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "lpf=1.0/25*np.ones((5,5))\n",
    "\n",
    "for i in range(0,1604):\n",
    "    \n",
    "    img_1=X_band_1[i].copy()\n",
    "    img_lpf_1=signal.convolve2d(img_1,lpf,mode='same')\n",
    "    img_Gx_1=signal.convolve2d(img_lpf_1,Gx_sobel,mode='same')\n",
    "    img_Gy_1=signal.convolve2d(img_lpf_1,Gy_sobel,mode='same')\n",
    "\n",
    "    img_2=X_band_2[i].copy()\n",
    "    img_lpf_2=signal.convolve2d(img_2,lpf,mode='same')\n",
    "    img_Gx_2=signal.convolve2d(img_lpf_2,Gx_sobel,mode='same')\n",
    "    img_Gy_2=signal.convolve2d(img_lpf_2,Gy_sobel,mode='same')\n",
    "\n",
    "    X_band_1[i] = np.hypot(img_Gx_1,img_Gy_1)\n",
    "    X_band_2[i] = np.hypot(img_Gx_2,img_Gy_2)\n",
    "'''\n",
    "\n",
    "\n",
    "X_band_3 = np.zeros((1604,75,75))\n",
    "for i in range(0,1604):\n",
    "    subt = abs(X_band_1[i]-X_band_2[i])\n",
    "    W1 = subt/subt.max()\n",
    "    W2=1-W1\n",
    "    X_band_3[i]=W1 * X_band_1[i]+W2 * X_band_2[i]\n",
    "\n",
    "\n",
    "# X_band_3=(X_band_1+X_band_2)/2  # 算术平均：尝试其他比例？\n",
    "#X_band_3=X_band_1 * 0.5858 + X_band_2 * 0.4142\n",
    "# 效果极差 X_band_3 = X_band_2 * X_band_2 / X_band_1\n",
    "#X_band_3 = np.sqrt(X_band_1 * X_band_2)  # 调和·平均: sqrt invalid number???\n",
    "#X_band_3 = X_band_1 / X_band_2\n",
    "# X_band_3=X_band_2 - X_band_1\n",
    "\n",
    "#from scipy import signal\n",
    "#X_band_3=signal.fftconvolve(X_band_1, X_band_2, mode = 'same')\n",
    "\n",
    "#X_band_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in train[\"inc_angle\"]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Generate the test data\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "\n",
    "'''\n",
    "for i in range(0,8424):\n",
    "    band_test_1_linear = decibel_to_linear(X_band_test_1[i])\n",
    "    band_test_2_linear = decibel_to_linear(X_band_test_2[i])\n",
    "\n",
    "    noise_var_test_1 = np.round(np.var(band_test_1_linear)*noise_var,10)\n",
    "    noise_var_test_2 = np.round(np.var(band_test_2_linear)*noise_var,10)\n",
    "\n",
    "    X_band_test_1[i] = linear_to_decibel(lee_filter(band_test_1_linear, windows[Wi], noise_var_test_1[Nj]))\n",
    "    X_band_test_2[i] = linear_to_decibel(lee_filter(band_test_2_linear, windows[Wi], noise_var_test_2[Nj]))\n",
    "\n",
    "\n",
    "for i in range(0,8424):\n",
    "    \n",
    "    img_1=X_band_test_1[i].copy()\n",
    "    img_lpf_1=signal.convolve2d(img_1,lpf,mode='same')\n",
    "    img_Gx_1=signal.convolve2d(img_lpf_1,Gx_sobel,mode='same')\n",
    "    img_Gy_1=signal.convolve2d(img_lpf_1,Gy_sobel,mode='same')\n",
    "\n",
    "    img_2=X_band_test_2[i].copy()\n",
    "    img_lpf_2=signal.convolve2d(img_2,lpf,mode='same')\n",
    "    img_Gx_2=signal.convolve2d(img_lpf_2,Gx_sobel,mode='same')\n",
    "    img_Gy_2=signal.convolve2d(img_lpf_2,Gy_sobel,mode='same')\n",
    "\n",
    "    X_band_test_1[i] = np.hypot(img_Gx_1,img_Gy_1)\n",
    "    X_band_test_2[i] = np.hypot(img_Gx_2,img_Gy_2)\n",
    "'''\n",
    "\n",
    "\n",
    "X_band_test_3 = np.zeros((8424,75,75))\n",
    "for i in range(0,8424):\n",
    "    subt = abs(X_band_test_1[i]-X_band_test_2[i])\n",
    "    W1 = subt/subt.max()\n",
    "    W2=1-W1\n",
    "    X_band_test_3[i]=W1 * X_band_test_1[i]+W2 * X_band_test_2[i]\n",
    "\n",
    "# X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "#X_band_test_3=X_band_test_1 * 0.5858 + X_band_test_2 * 0.4142\n",
    "#X_band_test_3 = X_band_test_2 * X_band_test_2 / X_band_test_1\n",
    "#X_band_test_3 = np.sqrt(X_band_test_1*X_band_test_2)\n",
    "#X_band_test_3=signal.fftconvolve(X_band_test_1, X_band_test_2, mode = 'same')\n",
    "#X_band_test_3 = X_band_test_1 / X_band_test_2\n",
    "# X_band_test_3 = X_band_test_2 - X_band_test_1\n",
    "\n",
    "#X_band_test_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in test[\"inc_angle\"]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_band_1=X_band_1+40\n",
    "X_band_2=X_band_2+40\n",
    "X_band_3=X_band_3+40\n",
    "X_band_test_1=X_band_test_1+40\n",
    "X_band_test_2=X_band_test_2+40\n",
    "X_band_test_3=X_band_test_3+40\n",
    "\n",
    "NX_band_1=np.zeros_like(X_band_1)\n",
    "NX_band_2=np.zeros_like(X_band_2)\n",
    "NX_band_3=np.zeros_like(X_band_3)\n",
    "NX_band_test_1=np.zeros_like(X_band_test_1)\n",
    "NX_band_test_2=np.zeros_like(X_band_test_2)\n",
    "NX_band_test_3=np.zeros_like(X_band_test_3)\n",
    "\n",
    "for i in range(0,X_band_1.shape[0]):\n",
    "    a=X_band_1[i]\n",
    "    b=X_band_2[i]\n",
    "    c=X_band_3[i]\n",
    "    minlist1=[]\n",
    "    maxlist1=[]\n",
    "    minlist2=[]\n",
    "    maxlist2=[]\n",
    "    minlist3=[]\n",
    "    maxlist3=[]\n",
    "    for j in range(len(a)):\n",
    "        minlist1.append(min(a[j]))\n",
    "        maxlist1.append(max(a[j]))\n",
    "        minlist2.append(min(b[j]))\n",
    "        maxlist2.append(max(b[j]))\n",
    "        minlist3.append(min(b[j]))\n",
    "        maxlist3.append(max(b[j]))\n",
    "\n",
    "    minvalue1=min(minlist1)\n",
    "    maxvalue1=max(maxlist1)\n",
    "    minvalue2=min(minlist2)\n",
    "    maxvalue2=max(maxlist2)\n",
    "    minvalue3=min(minlist3)\n",
    "    maxvalue3=max(maxlist3)\n",
    "    \n",
    "\n",
    "    NX_band_1[i]=((X_band_1[i]-minvalue1)**2/(maxvalue1-minvalue1))+minvalue1\n",
    "    NX_band_2[i]=((X_band_2[i]-minvalue2)**2/(maxvalue2-minvalue2))+minvalue2\n",
    "    NX_band_3[i]=((X_band_3[i]-minvalue3)**2/(maxvalue3-minvalue3))+minvalue3\n",
    "\n",
    "for i in range(0,X_band_test_1.shape[0]):\n",
    "    a=X_band_test_1[i]\n",
    "    b=X_band_test_2[i]\n",
    "    c=X_band_test_3[i]\n",
    "    minlist1=[]\n",
    "    maxlist1=[]\n",
    "    minlist2=[]\n",
    "    maxlist2=[]\n",
    "    minlist3=[]\n",
    "    maxlist3=[]\n",
    "    for j in range(len(a)):\n",
    "        minlist1.append(min(a[j]))\n",
    "        maxlist1.append(max(a[j]))\n",
    "        minlist2.append(min(b[j]))\n",
    "        maxlist2.append(max(b[j]))\n",
    "        minlist3.append(min(b[j]))\n",
    "        maxlist3.append(max(b[j]))\n",
    "\n",
    "    minvalue1=min(minlist1)\n",
    "    maxvalue1=max(maxlist1)\n",
    "    minvalue2=min(minlist2)\n",
    "    maxvalue2=max(maxlist2)\n",
    "    minvalue3=min(minlist3)\n",
    "    maxvalue3=max(maxlist3)\n",
    "\n",
    "    NX_band_test_1[i]=((X_band_test_1[i]-minvalue1)**2/(maxvalue1-minvalue1))+minvalue1\n",
    "    NX_band_test_2[i]=((X_band_test_2[i]-minvalue2)**2/(maxvalue2-minvalue2))+minvalue2\n",
    "    NX_band_test_3[i]=((X_band_test_3[i]-minvalue3)**2/(maxvalue3-minvalue3))+minvalue3\n",
    "\n",
    "X_band_1=NX_band_1\n",
    "X_band_2=NX_band_2\n",
    "X_band_3=(NX_band_1-NX_band_2)*2\n",
    "X_band_test_1=NX_band_test_1\n",
    "X_band_test_2=NX_band_test_2\n",
    "X_band_test_3=(NX_band_test_1-NX_band_test_2)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n",
    "                          , X_band_2[:, :, :, np.newaxis]\n",
    "                         , X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , X_band_test_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "# del train;del test;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理：提取一些统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stats(data,label=1):\n",
    "    data['max'+str(label)] = [np.max(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['min'+str(label)] = [np.min(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['minpos'+str(label)] = [np.argmin(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['med'+str(label)] = [np.median(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['std'+str(label)] = [np.std(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['mean'+str(label)] = [np.mean(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in data['band_'+str(label)] ]\n",
    "    data['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in data['band_'+str(label)] ]\n",
    "    data['mid50_'+str(label)] = data['p75_'+str(label)]-data['p25_'+str(label)]\n",
    "\n",
    "    return data\n",
    "\n",
    "train = get_stats(train,1)\n",
    "train = get_stats(train,2)\n",
    "X_min1 = train['min1']\n",
    "X_max1 = train['max1']\n",
    "X_std1 = train['std1']\n",
    "X_med1 = train['med1']\n",
    "X_mean1 = train['mean1']\n",
    "X_min2 = train['min2']\n",
    "X_max2 = train['max2']\n",
    "X_std2 = train['std2']\n",
    "X_med2 = train['med2']\n",
    "X_mean2 = train['mean2']\n",
    "\n",
    "test = get_stats(test,1)\n",
    "test = get_stats(test,2)\n",
    "X_test_min1 = test['min1']\n",
    "X_test_max1 = test['max1']\n",
    "X_test_std1 = test['std1']\n",
    "X_test_med1 = test['med1']\n",
    "X_test_mean1 = test['mean1']\n",
    "X_test_min2 = test['min2']\n",
    "X_test_max2 = test['max2']\n",
    "X_test_std2 = test['std2']\n",
    "X_test_med2 = test['med2']\n",
    "X_test_mean2 = test['mean2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Keras.\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "#from keras.optimizers import rmsprop\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# from keras.applications.vgg19 import preprocess_input\n",
    "#from keras.applications.xception import preprocess_input\n",
    "#from keras.applications.inception_v3 import preprocess_input\n",
    "# ResNet\n",
    "\n",
    "#Data Aug for multi-input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 批处理大小的设置\n",
    "# batch_size=64\n",
    "batch_size=64\n",
    "\n",
    "# Define the image transformations here\n",
    "# 原始旋转范围10\n",
    "#gen = ImageDataGenerator(horizontal_flip = True,\n",
    "#                         vertical_flip = True,\n",
    "#                         width_shift_range = 0.,\n",
    "#                         height_shift_range = 0.,\n",
    "#                         channel_shift_range=0,\n",
    "#                         zoom_range = 0.2,\n",
    "#                         rotation_range = 10)\n",
    "\n",
    "\n",
    "\n",
    "# aug1\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "#                          samplewise_center=True,\n",
    "#                          samplewise_std_normalization=True,\n",
    "                         width_shift_range = 0.,\n",
    "                         height_shift_range = 0.,\n",
    "                         channel_shift_range=0.,\n",
    "                         zoom_range = 0.2,\n",
    "                         rotation_range = 10)\n",
    "\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_twelve_inputs(X1, X2, X3,X4,X5,X6,X7,X8,X9,X10,X11,X12, y):\n",
    "# def gen_flow_for_twelve_inputs(X1, y):\n",
    "    gseed = 93 # 原来55\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=gseed)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=gseed)\n",
    "    genX3 = gen.flow(X1,X3, batch_size=batch_size,seed=gseed)\n",
    "    genX4 = gen.flow(X1,X4, batch_size=batch_size,seed=gseed)\n",
    "    genX5 = gen.flow(X1,X5, batch_size=batch_size,seed=gseed)\n",
    "    genX6 = gen.flow(X1,X6, batch_size=batch_size,seed=gseed)\n",
    "    genX7 = gen.flow(X1,X7, batch_size=batch_size,seed=gseed)\n",
    "    genX8 = gen.flow(X1,X8, batch_size=batch_size,seed=gseed)\n",
    "    genX9 = gen.flow(X1,X9, batch_size=batch_size,seed=gseed)\n",
    "    genX10 = gen.flow(X1,X10, batch_size=batch_size,seed=gseed)\n",
    "    genX11 = gen.flow(X1,X11, batch_size=batch_size,seed=gseed)\n",
    "    genX12 = gen.flow(X1,X12, batch_size=batch_size,seed=gseed)\n",
    "#     genX13 = gen.flow(X1,X13, batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            \n",
    "            X2i = genX2.next()\n",
    "            X3i = genX3.next()\n",
    "            X4i = genX4.next()\n",
    "            X5i = genX5.next()\n",
    "            X6i = genX6.next()\n",
    "            X7i = genX7.next()\n",
    "            X8i = genX8.next()\n",
    "            X9i = genX9.next()\n",
    "            X10i = genX10.next()\n",
    "            X11i = genX11.next()\n",
    "            X12i = genX12.next()\n",
    "            \n",
    "#             X13i = genX13.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1],X3i[1],X4i[1],X5i[1],X6i[1],X7i[1],\n",
    "                  X8i[1],X9i[1],X10i[1],X11i[1],X12i[1]], X1i[1]\n",
    "#             yield [X1i[0]],X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "def get_callbacks(filepath, patience=2):\n",
    "   es = EarlyStopping('val_loss', patience=10, mode=\"min\")\n",
    "   msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "   return [es, msave]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "def getVggAngleModel():\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "    input_3 = Input(shape=[1], name=\"min1\")\n",
    "    input_4 = Input(shape=[1], name=\"max1\")\n",
    "    input_5 = Input(shape=[1], name=\"std1\")\n",
    "    input_6 = Input(shape=[1], name=\"med1\")\n",
    "    input_7 = Input(shape=[1], name=\"mean1\")\n",
    "    input_8 = Input(shape=[1], name=\"min2\")\n",
    "    input_9 = Input(shape=[1], name=\"max2\")\n",
    "    input_10 = Input(shape=[1], name=\"std2\")\n",
    "    input_11 = Input(shape=[1], name=\"med2\")\n",
    "    input_12 = Input(shape=[1], name=\"mean2\")\n",
    "#     input_13 = Input(shape=[1], name=\"size\")\n",
    "    \n",
    "    angle_layer = Dense(1, )(input_2)\n",
    "    min1_layer = Dense(1, )(input_3)\n",
    "    max1_layer = Dense(1, )(input_4)\n",
    "    std1_layer = Dense(1, )(input_5)\n",
    "    med1_layer = Dense(1, )(input_6)\n",
    "    mean1_layer = Dense(1, )(input_7)\n",
    "    min2_layer = Dense(1, )(input_8)\n",
    "    max2_layer = Dense(1, )(input_9)\n",
    "    std2_layer = Dense(1, )(input_10)\n",
    "    med2_layer = Dense(1, )(input_11)\n",
    "    mean2_layer = Dense(1, )(input_12)\n",
    "#     size_layer = Dense(1, )(input_13)\n",
    "    \n",
    "    \n",
    "    # VGG16换成其他模型？？\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, \n",
    "                 input_shape=X_train.shape[1:], classes=1)\n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "#     x=Flatten()(x)\n",
    "    \n",
    "    \n",
    "    # mobile net\n",
    "#     base_model2 = keras.applications.mobilenet.MobileNet(weights=None, alpha=0.9,input_tensor = base_model.input,include_top=False, input_shape=X_train.shape[1:])\n",
    "#     base_model2 = keras.applications.mobilenet.MobileNet(weights=None, alpha=0.9,\n",
    "#                                                          include_top=False, input_shape=X_train.shape[1:])\n",
    "#     base_model2 = Xception(weights='imagenet', include_top=False, input_tensor = base_model.input,\n",
    "#                  input_shape=X_train.shape[1:], classes=1)\n",
    "#     x2 = base_model2.output\n",
    "#     x2 = GlobalMaxPooling2D()(x2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    merge_one = concatenate([x, angle_layer,\n",
    "                             min1_layer,max1_layer,std1_layer,med1_layer,mean1_layer,\n",
    "                            min2_layer,max2_layer,std2_layer,med2_layer,mean2_layer])\n",
    "    \n",
    "#     merge_one = x\n",
    "    merge_one = Dense(512, activation='relu', name='fc2')(merge_one)#原来\n",
    "#     merge_one = Dense(1024, activation='relu', name='fc2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one) # 参数原来0.3\n",
    "    merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n",
    "#     merge_one = Dense(256, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=[base_model.input, input_2,input_3,input_4,input_5,input_6,\n",
    "                        input_7,input_8,input_9,input_10, input_11,input_12], \n",
    "                  output=predictions)\n",
    "#     model = Model(input=[base_model.input], output=predictions)\n",
    "#     model = Model(input=[input_tensor, input_2], output=predictions)\n",
    "    \n",
    "    # 使用不同的优化\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adagrad = Adagrad(lr = 1e-3, epsilon = 1e-6)\n",
    "    rmsprop = RMSprop(lr=1e-3, rho = 0.9, epsilon=1e-6)\n",
    "    adadelta = Adadelta(lr=1e-3, rho=0.95, epsilon=1e-06)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    adamax = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    nadam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "    \n",
    "    # 更换loss\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using K-fold Cross Validation with Data Augmentation.\n",
    "def myAngleCV(X_train, X_angle, \n",
    "              X_min1,X_max1,X_std1,X_med1,X_mean1,\n",
    "              X_min2,X_max2,X_std2,X_med2,X_mean2,\n",
    "              X_test):\n",
    "    # K-折交叉验证\n",
    "    K=4\n",
    "    \n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=2017).split(X_train, target_train)) # 原16\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    \n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "        #Angle\n",
    "        X_angle_cv=X_angle[train_idx]\n",
    "        X_angle_hold=X_angle[test_idx]\n",
    "        \n",
    "        X_min1_cv=X_min1[train_idx]\n",
    "        X_min1_hold=X_min1[test_idx]\n",
    "        \n",
    "        X_max1_cv=X_max1[train_idx]\n",
    "        X_max1_hold=X_max1[test_idx]\n",
    "        \n",
    "        X_std1_cv=X_std1[train_idx]\n",
    "        X_std1_hold=X_std1[test_idx]\n",
    "        \n",
    "        X_med1_cv=X_med1[train_idx]\n",
    "        X_med1_hold=X_med1[test_idx]\n",
    "        \n",
    "        X_mean1_cv=X_mean1[train_idx]\n",
    "        X_mean1_hold=X_mean1[test_idx]\n",
    "        \n",
    "        X_min2_cv=X_min2[train_idx]\n",
    "        X_min2_hold=X_min2[test_idx]\n",
    "        \n",
    "        X_max2_cv=X_max2[train_idx]\n",
    "        X_max2_hold=X_max2[test_idx]\n",
    "        \n",
    "        X_std2_cv=X_std2[train_idx]\n",
    "        X_std2_hold=X_std2[test_idx]\n",
    "        \n",
    "        X_med2_cv=X_med2[train_idx]\n",
    "        X_med2_hold=X_med2[test_idx]\n",
    "        \n",
    "        X_mean2_cv=X_mean2[train_idx]\n",
    "        X_mean2_hold=X_mean2[test_idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"./model/%s_aug_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_twelve_inputs(X_train_cv, X_angle_cv, \n",
    "                                              X_min1_cv,X_max1_cv,X_std1_cv,X_med1_cv,X_mean1_cv,\n",
    "                                              X_min2_cv,X_max2_cv,X_std2_cv,X_med2_cv,X_mean2_cv,\n",
    "                                              y_train_cv)\n",
    "#         gen_flow = gen_flow_for_twelve_inputs(X_train_cv,y_train_cv)\n",
    "    \n",
    "        galaxyModel= getVggAngleModel()\n",
    "        \n",
    "#         gen_flow1=[gen_flow,X_angle_cv, X_min1_cv,X_max1_cv,X_std1_cv,X_med1_cv,X_mean1_cv,\n",
    "#                    X_min2_cv,X_max2_cv,X_std2_cv,X_med2_cv,X_mean2_cv]\n",
    "        \n",
    "        # 调整训练参数\n",
    "        galaxyModel.fit_generator(\n",
    "                gen_flow,\n",
    "#                 steps_per_epoch=24,\n",
    "                steps_per_epoch = len(X_train_cv)//batch_size,\n",
    "                #steps_per_epoch=100,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=([X_holdout,X_angle_hold,\n",
    "                                 X_min1_hold,X_max1_hold,X_std1_hold,X_med1_hold,X_mean1_hold,\n",
    "                                 X_min2_hold,X_max2_hold,X_std2_hold,X_med2_hold,X_mean2_hold], \n",
    "                                 Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        galaxyModel.load_weights(filepath=file_path)\n",
    "        #Getting Training Score\n",
    "        score = galaxyModel.evaluate([X_train_cv,X_angle_cv,\n",
    "                                     X_min1_cv,X_max1_cv,X_std1_cv,X_med1_cv,X_mean1_cv,\n",
    "                                     X_min2_cv,X_max2_cv,X_std2_cv,X_med2_cv,X_mean2_cv], \n",
    "                                     y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        #Getting Test Score\n",
    "        score = galaxyModel.evaluate([X_holdout,X_angle_hold,\n",
    "                                     X_min1_hold,X_max1_hold,X_std1_hold,X_med1_hold,X_mean1_hold,\n",
    "                                     X_min2_hold,X_max2_hold,X_std2_hold,X_med2_hold,X_mean2_hold], \n",
    "                                     Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.\n",
    "        pred_valid=galaxyModel.predict([X_holdout,X_angle_hold,\n",
    "                                       X_min1_hold,X_max1_hold,X_std1_hold,X_med1_hold,X_mean1_hold,\n",
    "                                       X_min2_hold,X_max2_hold,X_std2_hold,X_med2_hold,X_mean2_hold])\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "        temp_test=galaxyModel.predict([X_test, X_test_angle,\n",
    "                                      X_test_min1,X_test_max1,X_test_std1,X_test_med1,X_test_mean1,\n",
    "                                      X_test_min2,X_test_max2,X_test_std2,X_test_med2,X_test_mean2])\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train Scores\n",
    "        temp_train=galaxyModel.predict([X_train, X_angle,\n",
    "                                       X_min1,X_max1,X_std1,X_med1,X_mean1,\n",
    "                                       X_min2,X_max2,X_std2,X_med2,X_mean2])\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation= ',log_loss(target_train, y_train_pred_log))\n",
    "    print(' Test Log Loss Validation= ',log_loss(target_train, y_valid_pred_log))\n",
    "    return y_valid_pred_log,y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayden/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 41s - loss: 1.1832 - acc: 0.5686 - val_loss: 0.6659 - val_acc: 0.6517\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 37s - loss: 0.7185 - acc: 0.6379 - val_loss: 0.4340 - val_acc: 0.7935\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 30s - loss: 0.5684 - acc: 0.7158 - val_loss: 0.3068 - val_acc: 0.8607\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 30s - loss: 0.5420 - acc: 0.7297 - val_loss: 0.3451 - val_acc: 0.8184\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 31s - loss: 0.3909 - acc: 0.8136 - val_loss: 0.2718 - val_acc: 0.8781\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 31s - loss: 0.3595 - acc: 0.8175 - val_loss: 0.2421 - val_acc: 0.9005\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 30s - loss: 0.2775 - acc: 0.8818 - val_loss: 0.2422 - val_acc: 0.8930\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 30s - loss: 0.2822 - acc: 0.8787 - val_loss: 0.2499 - val_acc: 0.8980\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 30s - loss: 0.2800 - acc: 0.8734 - val_loss: 0.3071 - val_acc: 0.8408\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 31s - loss: 0.2803 - acc: 0.8694 - val_loss: 0.2152 - val_acc: 0.9204\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 31s - loss: 0.2535 - acc: 0.8983 - val_loss: 0.2115 - val_acc: 0.9104\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 29s - loss: 0.2467 - acc: 0.8831 - val_loss: 0.2368 - val_acc: 0.9030\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 29s - loss: 0.2693 - acc: 0.8840 - val_loss: 0.2278 - val_acc: 0.8980\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 30s - loss: 0.2199 - acc: 0.9064 - val_loss: 0.2124 - val_acc: 0.9080\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 31s - loss: 0.2084 - acc: 0.9092 - val_loss: 0.2118 - val_acc: 0.9129\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 31s - loss: 0.2267 - acc: 0.9024 - val_loss: 0.1979 - val_acc: 0.9104\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 31s - loss: 0.2037 - acc: 0.9155 - val_loss: 0.2518 - val_acc: 0.8881\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 30s - loss: 0.2368 - acc: 0.9016 - val_loss: 0.2034 - val_acc: 0.9204\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 31s - loss: 0.2166 - acc: 0.9172 - val_loss: 0.2029 - val_acc: 0.9179\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 31s - loss: 0.2298 - acc: 0.9010 - val_loss: 0.1814 - val_acc: 0.9229\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 30s - loss: 0.2080 - acc: 0.9131 - val_loss: 0.2126 - val_acc: 0.9129\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 11s - loss: 0.1977 - acc: 0.9270 - val_loss: 0.2057 - val_acc: 0.9229\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 11s - loss: 0.2019 - acc: 0.9200 - val_loss: 0.1873 - val_acc: 0.9254\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 10s - loss: 0.1807 - acc: 0.9233 - val_loss: 0.2092 - val_acc: 0.9005\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 10s - loss: 0.2129 - acc: 0.9116 - val_loss: 0.2388 - val_acc: 0.8980\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 12s - loss: 0.1719 - acc: 0.9307 - val_loss: 0.1851 - val_acc: 0.9279\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 13s - loss: 0.1746 - acc: 0.9274 - val_loss: 0.1838 - val_acc: 0.9229\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 12s - loss: 0.1707 - acc: 0.9338 - val_loss: 0.2474 - val_acc: 0.9080\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 13s - loss: 0.1854 - acc: 0.9244 - val_loss: 0.2200 - val_acc: 0.9055\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 12s - loss: 0.1591 - acc: 0.9450 - val_loss: 0.2461 - val_acc: 0.9104\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 13s - loss: 0.1726 - acc: 0.9291 - val_loss: 0.1806 - val_acc: 0.9303\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 10s - loss: 0.1627 - acc: 0.9305 - val_loss: 0.1869 - val_acc: 0.9303\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 10s - loss: 0.1714 - acc: 0.9361 - val_loss: 0.2268 - val_acc: 0.9080\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 11s - loss: 0.1750 - acc: 0.9342 - val_loss: 0.1952 - val_acc: 0.9254\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 13s - loss: 0.1625 - acc: 0.9287 - val_loss: 0.2233 - val_acc: 0.9080\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 12s - loss: 0.1688 - acc: 0.9348 - val_loss: 0.2094 - val_acc: 0.9254\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 12s - loss: 0.1641 - acc: 0.9339 - val_loss: 0.2325 - val_acc: 0.9005\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 12s - loss: 0.1377 - acc: 0.9485 - val_loss: 0.2232 - val_acc: 0.9104\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 12s - loss: 0.1547 - acc: 0.9384 - val_loss: 0.2097 - val_acc: 0.9204\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 12s - loss: 0.1371 - acc: 0.9424 - val_loss: 0.2721 - val_acc: 0.8980\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 12s - loss: 0.1516 - acc: 0.9379 - val_loss: 0.1956 - val_acc: 0.9204\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 12s - loss: 0.1522 - acc: 0.9385 - val_loss: 0.2010 - val_acc: 0.9154\n",
      "Train loss: 0.115509056435\n",
      "Train accuracy: 0.957570715474\n",
      "Test loss: 0.180592123845\n",
      "Test accuracy: 0.930348260486\n",
      "\n",
      "===================FOLD= 1\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 15s - loss: 1.1445 - acc: 0.5868 - val_loss: 0.7407 - val_acc: 0.5387\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 10s - loss: 0.7101 - acc: 0.6527 - val_loss: 0.4422 - val_acc: 0.8030\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 13s - loss: 0.5409 - acc: 0.7382 - val_loss: 0.3557 - val_acc: 0.8080\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 12s - loss: 0.4362 - acc: 0.8142 - val_loss: 0.3943 - val_acc: 0.7756\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 13s - loss: 0.3276 - acc: 0.8517 - val_loss: 0.2865 - val_acc: 0.8678\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 12s - loss: 0.3223 - acc: 0.8600 - val_loss: 0.2890 - val_acc: 0.8678\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 12s - loss: 0.2775 - acc: 0.8804 - val_loss: 0.3059 - val_acc: 0.8579\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 13s - loss: 0.2725 - acc: 0.8791 - val_loss: 0.2786 - val_acc: 0.8803\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 12s - loss: 0.2779 - acc: 0.8784 - val_loss: 0.2688 - val_acc: 0.8853\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 12s - loss: 0.2535 - acc: 0.8971 - val_loss: 0.2480 - val_acc: 0.8778\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 12s - loss: 0.2391 - acc: 0.9041 - val_loss: 0.2570 - val_acc: 0.8928\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 12s - loss: 0.2599 - acc: 0.8951 - val_loss: 0.2756 - val_acc: 0.8978\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 12s - loss: 0.2233 - acc: 0.9030 - val_loss: 0.2490 - val_acc: 0.8803\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 12s - loss: 0.2358 - acc: 0.9025 - val_loss: 0.2732 - val_acc: 0.9027\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 13s - loss: 0.2392 - acc: 0.9038 - val_loss: 0.2326 - val_acc: 0.9052\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 13s - loss: 0.2147 - acc: 0.8991 - val_loss: 0.2278 - val_acc: 0.9052\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 10s - loss: 0.2067 - acc: 0.9140 - val_loss: 0.2279 - val_acc: 0.9002\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 10s - loss: 0.2043 - acc: 0.9166 - val_loss: 0.2417 - val_acc: 0.8953\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 11s - loss: 0.1947 - acc: 0.9136 - val_loss: 0.2622 - val_acc: 0.8803\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 13s - loss: 0.1922 - acc: 0.9262 - val_loss: 0.2278 - val_acc: 0.8978\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 12s - loss: 0.1825 - acc: 0.9314 - val_loss: 0.2272 - val_acc: 0.9077\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 12s - loss: 0.1992 - acc: 0.9203 - val_loss: 0.2859 - val_acc: 0.8953\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 12s - loss: 0.2066 - acc: 0.9216 - val_loss: 0.2474 - val_acc: 0.8903\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 13s - loss: 0.1890 - acc: 0.9242 - val_loss: 0.2213 - val_acc: 0.9152\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 12s - loss: 0.1971 - acc: 0.9231 - val_loss: 0.2240 - val_acc: 0.9002\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 12s - loss: 0.1749 - acc: 0.9303 - val_loss: 0.2887 - val_acc: 0.8953\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 12s - loss: 0.1745 - acc: 0.9301 - val_loss: 0.2304 - val_acc: 0.9027\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 12s - loss: 0.1950 - acc: 0.9262 - val_loss: 0.2396 - val_acc: 0.8853\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 12s - loss: 0.1585 - acc: 0.9355 - val_loss: 0.2650 - val_acc: 0.9077\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 12s - loss: 0.1504 - acc: 0.9431 - val_loss: 0.2428 - val_acc: 0.9002\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 12s - loss: 0.1535 - acc: 0.9370 - val_loss: 0.2587 - val_acc: 0.9002\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 13s - loss: 0.1639 - acc: 0.9375 - val_loss: 0.2141 - val_acc: 0.8953\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 12s - loss: 0.1868 - acc: 0.9253 - val_loss: 0.2444 - val_acc: 0.8953\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 12s - loss: 0.1561 - acc: 0.9366 - val_loss: 0.2588 - val_acc: 0.9027\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 12s - loss: 0.1868 - acc: 0.9158 - val_loss: 0.2212 - val_acc: 0.8928\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 12s - loss: 0.1447 - acc: 0.9433 - val_loss: 0.2510 - val_acc: 0.8878\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 12s - loss: 0.1318 - acc: 0.9444 - val_loss: 0.2492 - val_acc: 0.8878\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 12s - loss: 0.1290 - acc: 0.9518 - val_loss: 0.2381 - val_acc: 0.9027\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 13s - loss: 0.1749 - acc: 0.9271 - val_loss: 0.2342 - val_acc: 0.9002\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 12s - loss: 0.1644 - acc: 0.9366 - val_loss: 0.2380 - val_acc: 0.8878\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 12s - loss: 0.1423 - acc: 0.9416 - val_loss: 0.2768 - val_acc: 0.8978\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 12s - loss: 0.1334 - acc: 0.9488 - val_loss: 0.2430 - val_acc: 0.8978\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 12s - loss: 0.1378 - acc: 0.9520 - val_loss: 0.2338 - val_acc: 0.9027\n",
      "Train loss: 0.117316766285\n",
      "Train accuracy: 0.96009975097\n",
      "Test loss: 0.214080040331\n",
      "Test accuracy: 0.895261848062\n",
      "\n",
      "===================FOLD= 2\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 13s - loss: 0.8688 - acc: 0.5903 - val_loss: 0.4792 - val_acc: 0.7756\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 10s - loss: 0.6804 - acc: 0.6674 - val_loss: 0.4904 - val_acc: 0.7855\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 12s - loss: 0.5185 - acc: 0.7469 - val_loss: 0.3209 - val_acc: 0.8653\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 13s - loss: 0.4119 - acc: 0.8151 - val_loss: 0.2766 - val_acc: 0.8803\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 13s - loss: 0.3460 - acc: 0.8376 - val_loss: 0.2502 - val_acc: 0.8828\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 13s - loss: 0.3357 - acc: 0.8368 - val_loss: 0.2427 - val_acc: 0.8953\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 13s - loss: 0.2988 - acc: 0.8633 - val_loss: 0.2292 - val_acc: 0.9052\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 12s - loss: 0.2887 - acc: 0.8776 - val_loss: 0.2490 - val_acc: 0.8928\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 13s - loss: 0.2547 - acc: 0.8856 - val_loss: 0.2216 - val_acc: 0.9127\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 12s - loss: 0.2652 - acc: 0.8819 - val_loss: 0.2051 - val_acc: 0.9302\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 12s - loss: 0.2187 - acc: 0.8978 - val_loss: 0.2224 - val_acc: 0.9177\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 12s - loss: 0.2742 - acc: 0.8728 - val_loss: 0.2135 - val_acc: 0.9352\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 12s - loss: 0.2158 - acc: 0.9032 - val_loss: 0.2213 - val_acc: 0.9027\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 12s - loss: 0.2030 - acc: 0.9147 - val_loss: 0.2066 - val_acc: 0.9127\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 13s - loss: 0.2155 - acc: 0.9017 - val_loss: 0.2008 - val_acc: 0.9277\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 10s - loss: 0.2042 - acc: 0.9154 - val_loss: 0.2034 - val_acc: 0.9152\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 11s - loss: 0.1985 - acc: 0.9136 - val_loss: 0.1947 - val_acc: 0.9352\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 11s - loss: 0.1811 - acc: 0.9216 - val_loss: 0.1923 - val_acc: 0.9277\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 12s - loss: 0.1745 - acc: 0.9254 - val_loss: 0.1994 - val_acc: 0.9327\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 13s - loss: 0.1758 - acc: 0.9219 - val_loss: 0.1950 - val_acc: 0.9277\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 13s - loss: 0.1901 - acc: 0.9223 - val_loss: 0.1757 - val_acc: 0.9352\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 12s - loss: 0.2053 - acc: 0.9147 - val_loss: 0.2007 - val_acc: 0.9152\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 12s - loss: 0.1774 - acc: 0.9249 - val_loss: 0.2500 - val_acc: 0.8928\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 12s - loss: 0.2193 - acc: 0.9086 - val_loss: 0.1842 - val_acc: 0.9327\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 12s - loss: 0.1842 - acc: 0.9240 - val_loss: 0.1861 - val_acc: 0.9401\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 12s - loss: 0.1667 - acc: 0.9258 - val_loss: 0.1759 - val_acc: 0.9327\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 12s - loss: 0.1769 - acc: 0.9216 - val_loss: 0.1870 - val_acc: 0.9277\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 12s - loss: 0.1561 - acc: 0.9423 - val_loss: 0.1861 - val_acc: 0.9327\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 12s - loss: 0.1601 - acc: 0.9396 - val_loss: 0.1869 - val_acc: 0.9377\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 12s - loss: 0.1472 - acc: 0.9388 - val_loss: 0.2086 - val_acc: 0.9202\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 12s - loss: 0.1774 - acc: 0.9310 - val_loss: 0.1813 - val_acc: 0.9401\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 12s - loss: 0.1659 - acc: 0.9358 - val_loss: 0.1825 - val_acc: 0.9426\n",
      "Train loss: 0.126900944304\n",
      "Train accuracy: 0.954280964256\n",
      "Test loss: 0.175675303153\n",
      "Test accuracy: 0.935162097439\n",
      "\n",
      "===================FOLD= 3\n",
      "Epoch 1/100\n",
      "18/18 [==============================] - 14s - loss: 0.8686 - acc: 0.5877 - val_loss: 0.4573 - val_acc: 0.7775\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 10s - loss: 0.7567 - acc: 0.6558 - val_loss: 0.6162 - val_acc: 0.5750\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 12s - loss: 0.6791 - acc: 0.6307 - val_loss: 0.6194 - val_acc: 0.5925\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 12s - loss: 0.6689 - acc: 0.6001 - val_loss: 0.5871 - val_acc: 0.7100\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 13s - loss: 0.6349 - acc: 0.6403 - val_loss: 0.5703 - val_acc: 0.6375\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 13s - loss: 0.6116 - acc: 0.6633 - val_loss: 0.4961 - val_acc: 0.7675\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 13s - loss: 0.4709 - acc: 0.7824 - val_loss: 0.4225 - val_acc: 0.7850\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 13s - loss: 0.5135 - acc: 0.7407 - val_loss: 0.4028 - val_acc: 0.8125\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 12s - loss: 0.4090 - acc: 0.8091 - val_loss: 0.3268 - val_acc: 0.8550\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 13s - loss: 0.3350 - acc: 0.8468 - val_loss: 0.3075 - val_acc: 0.8575\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 13s - loss: 0.3016 - acc: 0.8710 - val_loss: 0.3021 - val_acc: 0.8700\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 13s - loss: 0.2978 - acc: 0.8663 - val_loss: 0.2688 - val_acc: 0.8825\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 10s - loss: 0.2521 - acc: 0.8952 - val_loss: 0.3083 - val_acc: 0.8700\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 11s - loss: 0.2759 - acc: 0.8929 - val_loss: 0.2558 - val_acc: 0.8825\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 11s - loss: 0.2660 - acc: 0.8910 - val_loss: 0.2656 - val_acc: 0.8825\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 13s - loss: 0.2313 - acc: 0.9018 - val_loss: 0.2386 - val_acc: 0.8925\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 12s - loss: 0.2241 - acc: 0.9043 - val_loss: 0.2785 - val_acc: 0.8725\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 12s - loss: 0.2205 - acc: 0.9145 - val_loss: 0.2399 - val_acc: 0.8875\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 13s - loss: 0.2440 - acc: 0.8903 - val_loss: 0.2394 - val_acc: 0.8825\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 12s - loss: 0.2081 - acc: 0.9193 - val_loss: 0.2409 - val_acc: 0.9000\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 12s - loss: 0.2132 - acc: 0.9139 - val_loss: 0.2415 - val_acc: 0.9000\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 12s - loss: 0.1958 - acc: 0.9196 - val_loss: 0.2504 - val_acc: 0.8950\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 12s - loss: 0.2331 - acc: 0.9098 - val_loss: 0.2491 - val_acc: 0.8950\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 13s - loss: 0.2009 - acc: 0.9223 - val_loss: 0.2340 - val_acc: 0.9050\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 12s - loss: 0.2043 - acc: 0.9213 - val_loss: 0.2727 - val_acc: 0.8825\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 13s - loss: 0.1973 - acc: 0.9191 - val_loss: 0.2195 - val_acc: 0.9100\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 12s - loss: 0.2038 - acc: 0.9211 - val_loss: 0.2609 - val_acc: 0.8750\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 12s - loss: 0.1733 - acc: 0.9301 - val_loss: 0.2360 - val_acc: 0.9025\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 12s - loss: 0.1692 - acc: 0.9406 - val_loss: 0.2256 - val_acc: 0.9125\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 12s - loss: 0.1904 - acc: 0.9247 - val_loss: 0.2338 - val_acc: 0.9075\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 12s - loss: 0.1741 - acc: 0.9367 - val_loss: 0.2365 - val_acc: 0.8975\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 12s - loss: 0.1726 - acc: 0.9275 - val_loss: 0.2411 - val_acc: 0.9000\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 12s - loss: 0.1625 - acc: 0.9283 - val_loss: 0.2474 - val_acc: 0.9025\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 12s - loss: 0.2061 - acc: 0.9153 - val_loss: 0.2140 - val_acc: 0.9025\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 12s - loss: 0.1504 - acc: 0.9404 - val_loss: 0.2289 - val_acc: 0.9175\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 12s - loss: 0.1586 - acc: 0.9371 - val_loss: 0.2639 - val_acc: 0.8975\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 12s - loss: 0.1662 - acc: 0.9362 - val_loss: 0.2369 - val_acc: 0.9075\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 12s - loss: 0.1622 - acc: 0.9345 - val_loss: 0.2311 - val_acc: 0.8925\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 13s - loss: 0.1566 - acc: 0.9375 - val_loss: 0.2460 - val_acc: 0.9025\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 12s - loss: 0.1623 - acc: 0.9376 - val_loss: 0.2204 - val_acc: 0.9100\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 12s - loss: 0.1363 - acc: 0.9470 - val_loss: 0.2206 - val_acc: 0.9025\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 12s - loss: 0.1384 - acc: 0.9430 - val_loss: 0.2326 - val_acc: 0.9125\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 12s - loss: 0.1538 - acc: 0.9402 - val_loss: 0.2855 - val_acc: 0.8750\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 12s - loss: 0.1248 - acc: 0.9534 - val_loss: 0.2600 - val_acc: 0.8975\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 12s - loss: 0.1552 - acc: 0.9384 - val_loss: 0.2406 - val_acc: 0.8875\n",
      "Train loss: 0.118319999018\n",
      "Train accuracy: 0.955149501661\n",
      "Test loss: 0.214032992125\n",
      "Test accuracy: 0.9025\n",
      "\n",
      " Train Log Loss Validation=  0.12634911673\n",
      " Test Log Loss Validation=  0.196074269655\n"
     ]
    }
   ],
   "source": [
    "train_preds,test_preds=myAngleCV(X_train, X_angle, \n",
    "                X_min1,X_max1,X_std1,X_med1,X_mean1,\n",
    "                X_min2,X_max2,X_std2,X_med2,X_mean2,\n",
    "                X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Submission for each day.\n",
    "full_predict=pd.Series(np.r_[train_preds,test_preds])\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['is_iceberg']=test_preds\n",
    "# full_predict.to_csv('full_preds_vgg3.csv',index=False)\n",
    "submission.to_csv('./submission/subVGG7.19.csv', index=False)\n",
    "# submission.to_csv('./submission/subMobile-6.2')\n",
    "# submission.to_csv('./submission/subVggMobile-10.3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
