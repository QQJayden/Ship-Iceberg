{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Xception - 加入各种特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "+ [Exploration & Transforming Images in Python](https://www.kaggle.com/muonneutrino/exploration-transforming-images-in-python)\n",
    "+ [Transfer Learning with VGG-16 CNN+AUG LB 0.1712](http://localhost:8888/notebooks/kaggle/ShipIceberg/TransferLearning-VGG.ipynb)\n",
    "+ [Despeckling Synthetic Aperture Radar (SAR) Images](https://www.kaggle.com/jgroff/despeckling-synthetic-aperture-radar-sar-images)\n",
    "+ [Submarineering. Size Matters](https://www.kaggle.com/submarineering/submarineering-size-matters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mandatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"../ShipIceberg/Data/train.json\")\n",
    "target_train=train['is_iceberg']\n",
    "\n",
    "# 一直cannot resolve memory block\n",
    "# test = pd.read_json(\"../ShipIceberg/Data/test.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../ShipIceberg/Data/test.json', 'r') as f:\n",
    "    test = json.load(f)\n",
    "    test=pd.DataFrame(test)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['inc_angle'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train=train['is_iceberg']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "X_angle=train['inc_angle']\n",
    "# test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "X_test_angle=test['inc_angle']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# implement functions to convert SAR data from decibel units to linear units and back again\n",
    "def decibel_to_linear(band):\n",
    "     # convert to linear units\n",
    "    return np.power(10,np.array(band)/10)\n",
    "\n",
    "def linear_to_decibel(band):\n",
    "    return 10*np.log10(band)\n",
    "\n",
    "# implement the Lee Filter for a band in an image already reshaped into the proper dimensions\n",
    "def lee_filter(band, window, var_noise = 0.25):\n",
    "        # band: SAR data to be despeckled (already reshaped into image dimensions)\n",
    "        # window: descpeckling filter window (tuple)\n",
    "        # default noise variance = 0.25\n",
    "        # assumes noise mean = 0\n",
    "    \n",
    "        mean_window = uniform_filter(band, window)\n",
    "        mean_sqr_window = uniform_filter(band**2, window)\n",
    "        var_window = mean_sqr_window - mean_window**2\n",
    "\n",
    "        weights = var_window / (var_window + var_noise)\n",
    "        band_filtered = mean_window + weights*(band - mean_window)\n",
    "        return band_filtered\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Isolation function.\n",
    "def iso(arr):\n",
    "    p = np.reshape(np.array(arr), [75,75]) >(np.mean(np.array(arr))+2*np.std(np.array(arr)))\n",
    "    return p * np.reshape(np.array(arr), [75,75])\n",
    "\n",
    "# Size in number of pixels of every isolated object.\n",
    "def size(arr):     \n",
    "    return np.sum(arr<-5)\n",
    "\n",
    "# Feature engineering iso1 and iso2.\n",
    "train['iso1'] = train.iloc[:, 0].apply(iso)\n",
    "train['iso2'] = train.iloc[:, 1].apply(iso)\n",
    "\n",
    "# train.head(5)\n",
    "\n",
    "'''\n",
    "# Feature engineering s1 s2 and size.\n",
    "train['s1'] = train.iloc[:,5].apply(size)\n",
    "train['s2'] = train.iloc[:,6].apply(size)\n",
    "train['size'] = train.s1+train.s2\n",
    "\n",
    "X_size = train['size']\n",
    "\n",
    "# Feature engineering iso1 and iso2.\n",
    "test['iso1'] = test.iloc[:, 0].apply(iso)\n",
    "test['iso2'] = test.iloc[:, 1].apply(iso)\n",
    "\n",
    "# test.head(5)\n",
    "\n",
    "test['s1'] = test.iloc[:,4].apply(size)\n",
    "test['s2'] = test.iloc[:,5].apply(size)\n",
    "test['size'] = test.s1+test.s2\n",
    "\n",
    "\n",
    "X_test_size = test['size']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "\n",
    "'''\n",
    "# 需要循环？？？\n",
    "# let's see if despeckling has any influence on the images\n",
    "# plot nine different instances with different windows and noise levels (band 1)\n",
    "windows = [2, 4, 8] # can be tuple too if not symetric\n",
    "noise_var = np.array([1, 2, 4])\n",
    "\n",
    "# 0-2\n",
    "Wi = 0\n",
    "Nj = 0\n",
    "\n",
    "for i in range(0,1604):\n",
    "    band_1_linear = decibel_to_linear(X_band_1[i])\n",
    "    band_2_linear = decibel_to_linear(X_band_2[i])\n",
    "\n",
    "    noise_var_1 = np.round(np.var(band_1_linear)*noise_var,10)\n",
    "    noise_var_2 = np.round(np.var(band_2_linear)*noise_var,10)\n",
    "\n",
    "    X_band_1[i] = linear_to_decibel(lee_filter(band_1_linear, windows[Wi], noise_var_1[Nj]))\n",
    "    X_band_2[i] = linear_to_decibel(lee_filter(band_2_linear, windows[Wi], noise_var_2[Nj]))\n",
    "\n",
    "\n",
    "# Sobel Operator & Averaging\n",
    "from scipy import signal\n",
    "\n",
    "Gx_sobel=np.array([[1,0,-1],[2,0,-2],[1,0,-1]])\n",
    "Gy_sobel=np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "lpf=1.0/25*np.ones((5,5))\n",
    "\n",
    "for i in range(0,1604):\n",
    "    \n",
    "    img_1=X_band_1[i].copy()\n",
    "    img_lpf_1=signal.convolve2d(img_1,lpf,mode='same')\n",
    "    img_Gx_1=signal.convolve2d(img_lpf_1,Gx_sobel,mode='same')\n",
    "    img_Gy_1=signal.convolve2d(img_lpf_1,Gy_sobel,mode='same')\n",
    "\n",
    "    img_2=X_band_2[i].copy()\n",
    "    img_lpf_2=signal.convolve2d(img_2,lpf,mode='same')\n",
    "    img_Gx_2=signal.convolve2d(img_lpf_2,Gx_sobel,mode='same')\n",
    "    img_Gy_2=signal.convolve2d(img_lpf_2,Gy_sobel,mode='same')\n",
    "\n",
    "    X_band_1[i] = np.hypot(img_Gx_1,img_Gy_1)\n",
    "    X_band_2[i] = np.hypot(img_Gx_2,img_Gy_2)\n",
    "'''\n",
    "\n",
    "\n",
    "X_band_3 = np.zeros((1604,75,75))\n",
    "for i in range(0,1604):\n",
    "    subt = abs(X_band_1[i]-X_band_2[i])\n",
    "    W1 = subt/subt.max()\n",
    "    W2=1-W1\n",
    "    X_band_3[i]=W1 * X_band_1[i]+W2 * X_band_2[i]\n",
    "\n",
    "\n",
    "# X_band_3=(X_band_1+X_band_2)/2  # 算术平均：尝试其他比例？\n",
    "#X_band_3=X_band_1 * 0.5858 + X_band_2 * 0.4142\n",
    "# 效果极差 X_band_3 = X_band_2 * X_band_2 / X_band_1\n",
    "#X_band_3 = np.sqrt(X_band_1 * X_band_2)  # 调和·平均: sqrt invalid number???\n",
    "#X_band_3 = X_band_1 / X_band_2\n",
    "# X_band_3=X_band_2 - X_band_1\n",
    "\n",
    "#from scipy import signal\n",
    "#X_band_3=signal.fftconvolve(X_band_1, X_band_2, mode = 'same')\n",
    "\n",
    "#X_band_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in train[\"inc_angle\"]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Generate the test data\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "\n",
    "'''\n",
    "for i in range(0,8424):\n",
    "    band_test_1_linear = decibel_to_linear(X_band_test_1[i])\n",
    "    band_test_2_linear = decibel_to_linear(X_band_test_2[i])\n",
    "\n",
    "    noise_var_test_1 = np.round(np.var(band_test_1_linear)*noise_var,10)\n",
    "    noise_var_test_2 = np.round(np.var(band_test_2_linear)*noise_var,10)\n",
    "\n",
    "    X_band_test_1[i] = linear_to_decibel(lee_filter(band_test_1_linear, windows[Wi], noise_var_test_1[Nj]))\n",
    "    X_band_test_2[i] = linear_to_decibel(lee_filter(band_test_2_linear, windows[Wi], noise_var_test_2[Nj]))\n",
    "\n",
    "\n",
    "for i in range(0,8424):\n",
    "    \n",
    "    img_1=X_band_test_1[i].copy()\n",
    "    img_lpf_1=signal.convolve2d(img_1,lpf,mode='same')\n",
    "    img_Gx_1=signal.convolve2d(img_lpf_1,Gx_sobel,mode='same')\n",
    "    img_Gy_1=signal.convolve2d(img_lpf_1,Gy_sobel,mode='same')\n",
    "\n",
    "    img_2=X_band_test_2[i].copy()\n",
    "    img_lpf_2=signal.convolve2d(img_2,lpf,mode='same')\n",
    "    img_Gx_2=signal.convolve2d(img_lpf_2,Gx_sobel,mode='same')\n",
    "    img_Gy_2=signal.convolve2d(img_lpf_2,Gy_sobel,mode='same')\n",
    "\n",
    "    X_band_test_1[i] = np.hypot(img_Gx_1,img_Gy_1)\n",
    "    X_band_test_2[i] = np.hypot(img_Gx_2,img_Gy_2)\n",
    "'''\n",
    "\n",
    "\n",
    "X_band_test_3 = np.zeros((8424,75,75))\n",
    "for i in range(0,8424):\n",
    "    subt = abs(X_band_test_1[i]-X_band_test_2[i])\n",
    "    W1 = subt/subt.max()\n",
    "    W2=1-W1\n",
    "    X_band_test_3[i]=W1 * X_band_test_1[i]+W2 * X_band_test_2[i]\n",
    "\n",
    "# X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "#X_band_test_3=X_band_test_1 * 0.5858 + X_band_test_2 * 0.4142\n",
    "#X_band_test_3 = X_band_test_2 * X_band_test_2 / X_band_test_1\n",
    "#X_band_test_3 = np.sqrt(X_band_test_1*X_band_test_2)\n",
    "#X_band_test_3=signal.fftconvolve(X_band_test_1, X_band_test_2, mode = 'same')\n",
    "#X_band_test_3 = X_band_test_1 / X_band_test_2\n",
    "# X_band_test_3 = X_band_test_2 - X_band_test_1\n",
    "\n",
    "#X_band_test_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in test[\"inc_angle\"]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_band_1=X_band_1+40\n",
    "X_band_2=X_band_2+40\n",
    "X_band_3=X_band_3+40\n",
    "X_band_test_1=X_band_test_1+40\n",
    "X_band_test_2=X_band_test_2+40\n",
    "X_band_test_3=X_band_test_3+40\n",
    "\n",
    "NX_band_1=np.zeros_like(X_band_1)\n",
    "NX_band_2=np.zeros_like(X_band_2)\n",
    "NX_band_3=np.zeros_like(X_band_3)\n",
    "NX_band_test_1=np.zeros_like(X_band_test_1)\n",
    "NX_band_test_2=np.zeros_like(X_band_test_2)\n",
    "NX_band_test_3=np.zeros_like(X_band_test_3)\n",
    "\n",
    "for i in range(0,X_band_1.shape[0]):\n",
    "    a=X_band_1[i]\n",
    "    b=X_band_2[i]\n",
    "    c=X_band_3[i]\n",
    "    minlist1=[]\n",
    "    maxlist1=[]\n",
    "    minlist2=[]\n",
    "    maxlist2=[]\n",
    "    minlist3=[]\n",
    "    maxlist3=[]\n",
    "    for j in range(len(a)):\n",
    "        minlist1.append(min(a[j]))\n",
    "        maxlist1.append(max(a[j]))\n",
    "        minlist2.append(min(b[j]))\n",
    "        maxlist2.append(max(b[j]))\n",
    "        minlist3.append(min(b[j]))\n",
    "        maxlist3.append(max(b[j]))\n",
    "\n",
    "    minvalue1=min(minlist1)\n",
    "    maxvalue1=max(maxlist1)\n",
    "    minvalue2=min(minlist2)\n",
    "    maxvalue2=max(maxlist2)\n",
    "    minvalue3=min(minlist3)\n",
    "    maxvalue3=max(maxlist3)\n",
    "    \n",
    "\n",
    "    NX_band_1[i]=((X_band_1[i]-minvalue1)**2/(maxvalue1-minvalue1))+minvalue1\n",
    "    NX_band_2[i]=((X_band_2[i]-minvalue2)**2/(maxvalue2-minvalue2))+minvalue2\n",
    "    NX_band_3[i]=((X_band_3[i]-minvalue3)**2/(maxvalue3-minvalue3))+minvalue3\n",
    "\n",
    "for i in range(0,X_band_test_1.shape[0]):\n",
    "    a=X_band_test_1[i]\n",
    "    b=X_band_test_2[i]\n",
    "    c=X_band_test_3[i]\n",
    "    minlist1=[]\n",
    "    maxlist1=[]\n",
    "    minlist2=[]\n",
    "    maxlist2=[]\n",
    "    minlist3=[]\n",
    "    maxlist3=[]\n",
    "    for j in range(len(a)):\n",
    "        minlist1.append(min(a[j]))\n",
    "        maxlist1.append(max(a[j]))\n",
    "        minlist2.append(min(b[j]))\n",
    "        maxlist2.append(max(b[j]))\n",
    "        minlist3.append(min(b[j]))\n",
    "        maxlist3.append(max(b[j]))\n",
    "\n",
    "    minvalue1=min(minlist1)\n",
    "    maxvalue1=max(maxlist1)\n",
    "    minvalue2=min(minlist2)\n",
    "    maxvalue2=max(maxlist2)\n",
    "    minvalue3=min(minlist3)\n",
    "    maxvalue3=max(maxlist3)\n",
    "\n",
    "    NX_band_test_1[i]=((X_band_test_1[i]-minvalue1)**2/(maxvalue1-minvalue1))+minvalue1\n",
    "    NX_band_test_2[i]=((X_band_test_2[i]-minvalue2)**2/(maxvalue2-minvalue2))+minvalue2\n",
    "    NX_band_test_3[i]=((X_band_test_3[i]-minvalue3)**2/(maxvalue3-minvalue3))+minvalue3\n",
    "\n",
    "X_band_1=NX_band_1\n",
    "X_band_2=NX_band_2\n",
    "X_band_3=(NX_band_1-NX_band_2)*2\n",
    "X_band_test_1=NX_band_test_1\n",
    "X_band_test_2=NX_band_test_2\n",
    "X_band_test_3=(NX_band_test_1-NX_band_test_2)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n",
    "                          , X_band_2[:, :, :, np.newaxis]\n",
    "                         , X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , X_band_test_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "# del train;del test;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理：提取一些统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stats(data,label=1):\n",
    "    data['max'+str(label)] = [np.max(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['min'+str(label)] = [np.min(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['minpos'+str(label)] = [np.argmin(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['med'+str(label)] = [np.median(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['std'+str(label)] = [np.std(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['mean'+str(label)] = [np.mean(np.array(x)) for x in data['band_'+str(label)] ]\n",
    "    data['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in data['band_'+str(label)] ]\n",
    "    data['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in data['band_'+str(label)] ]\n",
    "    data['mid50_'+str(label)] = data['p75_'+str(label)]-data['p25_'+str(label)]\n",
    "\n",
    "    return data\n",
    "\n",
    "train = get_stats(train,1)\n",
    "train = get_stats(train,2)\n",
    "X_min1 = train['min1']\n",
    "X_max1 = train['max1']\n",
    "X_std1 = train['std1']\n",
    "X_med1 = train['med1']\n",
    "X_mean1 = train['mean1']\n",
    "X_min2 = train['min2']\n",
    "X_max2 = train['max2']\n",
    "X_std2 = train['std2']\n",
    "X_med2 = train['med2']\n",
    "X_mean2 = train['mean2']\n",
    "\n",
    "test = get_stats(test,1)\n",
    "test = get_stats(test,2)\n",
    "X_test_min1 = test['min1']\n",
    "X_test_max1 = test['max1']\n",
    "X_test_std1 = test['std1']\n",
    "X_test_med1 = test['med1']\n",
    "X_test_mean1 = test['mean1']\n",
    "X_test_min2 = test['min2']\n",
    "X_test_max2 = test['max2']\n",
    "X_test_std2 = test['std2']\n",
    "X_test_med2 = test['med2']\n",
    "X_test_mean2 = test['mean2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Keras.\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "#from keras.optimizers import rmsprop\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "# from keras.applications.vgg19 import preprocess_input\n",
    "#from keras.applications.xception import preprocess_input\n",
    "#from keras.applications.inception_v3 import preprocess_input\n",
    "# ResNet\n",
    "\n",
    "#Data Aug for multi-input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 批处理大小的设置\n",
    "# batch_size=64\n",
    "batch_size=64\n",
    "\n",
    "# Define the image transformations here\n",
    "# 原始旋转范围10\n",
    "#gen = ImageDataGenerator(horizontal_flip = True,\n",
    "#                         vertical_flip = True,\n",
    "#                         width_shift_range = 0.,\n",
    "#                         height_shift_range = 0.,\n",
    "#                         channel_shift_range=0,\n",
    "#                         zoom_range = 0.2,\n",
    "#                         rotation_range = 10)\n",
    "\n",
    "\n",
    "\n",
    "# aug1\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "#                          samplewise_center=True,\n",
    "#                          samplewise_std_normalization=True,\n",
    "                         width_shift_range = 0.,\n",
    "                         height_shift_range = 0.,\n",
    "                         channel_shift_range=0.,\n",
    "                         zoom_range = 0.2,\n",
    "                         rotation_range = 10)\n",
    "\n",
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_twelve_inputs(X1, X2, X3,X4,X5,X6,X7,X8,X9,X10,X11,X12, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n",
    "    genX3 = gen.flow(X1,X3, batch_size=batch_size,seed=55)\n",
    "    genX4 = gen.flow(X1,X4, batch_size=batch_size,seed=55)\n",
    "    genX5 = gen.flow(X1,X5, batch_size=batch_size,seed=55)\n",
    "    genX6 = gen.flow(X1,X6, batch_size=batch_size,seed=55)\n",
    "    genX7 = gen.flow(X1,X7, batch_size=batch_size,seed=55)\n",
    "    genX8 = gen.flow(X1,X8, batch_size=batch_size,seed=55)\n",
    "    genX9 = gen.flow(X1,X9, batch_size=batch_size,seed=55)\n",
    "    genX10 = gen.flow(X1,X10, batch_size=batch_size,seed=55)\n",
    "    genX11 = gen.flow(X1,X11, batch_size=batch_size,seed=55)\n",
    "    genX12 = gen.flow(X1,X12, batch_size=batch_size,seed=55)\n",
    "#     genX13 = gen.flow(X1,X13, batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            X3i = genX3.next()\n",
    "            X4i = genX4.next()\n",
    "            X5i = genX5.next()\n",
    "            X6i = genX6.next()\n",
    "            X7i = genX7.next()\n",
    "            X8i = genX8.next()\n",
    "            X9i = genX9.next()\n",
    "            X10i = genX10.next()\n",
    "            X11i = genX11.next()\n",
    "            X12i = genX12.next()\n",
    "#             X13i = genX13.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1],X3i[1],X4i[1],X5i[1],X6i[1],X7i[1],\n",
    "                  X8i[1],X9i[1],X10i[1],X11i[1],X12i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "def get_callbacks(filepath, patience=2):\n",
    "   es = EarlyStopping('val_loss', patience=10, mode=\"min\")\n",
    "   msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "   return [es, msave]\n",
    "\n",
    "\n",
    "def getVggAngleModel():\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "    input_3 = Input(shape=[1], name=\"min1\")\n",
    "    input_4 = Input(shape=[1], name=\"max1\")\n",
    "    input_5 = Input(shape=[1], name=\"std1\")\n",
    "    input_6 = Input(shape=[1], name=\"med1\")\n",
    "    input_7 = Input(shape=[1], name=\"mean1\")\n",
    "    input_8 = Input(shape=[1], name=\"min2\")\n",
    "    input_9 = Input(shape=[1], name=\"max2\")\n",
    "    input_10 = Input(shape=[1], name=\"std2\")\n",
    "    input_11 = Input(shape=[1], name=\"med2\")\n",
    "    input_12 = Input(shape=[1], name=\"mean2\")\n",
    "#     input_13 = Input(shape=[1], name=\"size\")\n",
    "    \n",
    "    angle_layer = Dense(1, )(input_2)\n",
    "    min1_layer = Dense(1, )(input_3)\n",
    "    max1_layer = Dense(1, )(input_4)\n",
    "    std1_layer = Dense(1, )(input_5)\n",
    "    med1_layer = Dense(1, )(input_6)\n",
    "    mean1_layer = Dense(1, )(input_7)\n",
    "    min2_layer = Dense(1, )(input_8)\n",
    "    max2_layer = Dense(1, )(input_9)\n",
    "    std2_layer = Dense(1, )(input_10)\n",
    "    med2_layer = Dense(1, )(input_11)\n",
    "    mean2_layer = Dense(1, )(input_12)\n",
    "#     size_layer = Dense(1, )(input_13)\n",
    "    \n",
    "    # VGG16换成其他模型？？\n",
    "    base_model = Xception(weights='imagenet', include_top=False, \n",
    "                 input_shape=X_train.shape[1:], classes=1)\n",
    "#     x = base_model.get_layer('block5_pool').output\n",
    "    x = base_model.output\n",
    "    \n",
    "    '''\n",
    "    # BatchNorm 未形成conv bn scale relu 的block\n",
    "    input_tensor = Input(shape=X_train.shape[1:])\n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "#     x = base_model(bn)\n",
    "    x = base_model.get_layer('block5_pool')(bn)\n",
    "'''\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    merge_one = concatenate([x, angle_layer,\n",
    "                             min1_layer,max1_layer,std1_layer,med1_layer,mean1_layer,\n",
    "                            min2_layer,max2_layer,std2_layer,med2_layer,mean2_layer])\n",
    "    \n",
    "#     merge_one = x\n",
    "    merge_one = Dense(512, activation='relu', name='fc2')(merge_one)\n",
    "#     merge_one = Dense(1024, activation='relu', name='fc2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one) # 参数原来0.3\n",
    "    merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n",
    "#     merge_one = Dense(256, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=[base_model.input, input_2,input_3,input_4,input_5,input_6,\n",
    "                        input_7,input_8,input_9,input_10,input_11,input_12], \n",
    "                  output=predictions)\n",
    "#     model = Model(input=[base_model.input], output=predictions)\n",
    "#     model = Model(input=[input_tensor, input_2], output=predictions)\n",
    "    \n",
    "    # 使用不同的优化\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adagrad = Adagrad(lr = 1e-3, epsilon = 1e-6)\n",
    "    rmsprop = RMSprop(lr=1e-3, rho = 0.9, epsilon=1e-6)\n",
    "    adadelta = Adadelta(lr=1e-3, rho=0.95, epsilon=1e-06)\n",
    "    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    adamax = Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    nadam = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "    \n",
    "    # 更换loss\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Using K-fold Cross Validation with Data Augmentation.\n",
    "def myAngleCV(X_train, X_angle, \n",
    "              X_min1,X_max1,X_std1,X_med1,X_mean1,\n",
    "              X_min2,X_max2,X_std2,X_med2,X_mean2,\n",
    "              X_test):\n",
    "    # K-折交叉验证\n",
    "    K=3\n",
    "    \n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    \n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "        #Angle\n",
    "        X_angle_cv=X_angle[train_idx]\n",
    "        X_angle_hold=X_angle[test_idx]\n",
    "        \n",
    "        X_min1_cv=X_min1[train_idx]\n",
    "        X_min1_hold=X_min1[test_idx]\n",
    "        \n",
    "        X_max1_cv=X_max1[train_idx]\n",
    "        X_max1_hold=X_max1[test_idx]\n",
    "        \n",
    "        X_std1_cv=X_std1[train_idx]\n",
    "        X_std1_hold=X_std1[test_idx]\n",
    "        \n",
    "        X_med1_cv=X_med1[train_idx]\n",
    "        X_med1_hold=X_med1[test_idx]\n",
    "        \n",
    "        X_mean1_cv=X_mean1[train_idx]\n",
    "        X_mean1_hold=X_mean1[test_idx]\n",
    "        \n",
    "        X_min2_cv=X_min2[train_idx]\n",
    "        X_min2_hold=X_min2[test_idx]\n",
    "        \n",
    "        X_max2_cv=X_max2[train_idx]\n",
    "        X_max2_hold=X_max2[test_idx]\n",
    "        \n",
    "        X_std2_cv=X_std2[train_idx]\n",
    "        X_std2_hold=X_std2[test_idx]\n",
    "        \n",
    "        X_med2_cv=X_med2[train_idx]\n",
    "        X_med2_hold=X_med2[test_idx]\n",
    "        \n",
    "        X_mean2_cv=X_mean2[train_idx]\n",
    "        X_mean2_hold=X_mean2[test_idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"./model/%s_aug_xception_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_twelve_inputs(X_train_cv, X_angle_cv, \n",
    "                                              X_min1_cv,X_max1_cv,X_std1_cv,X_med1_cv,X_mean1_cv,\n",
    "                                              X_min2_cv,X_max2_cv,X_std2_cv,X_med2_cv,X_mean2_cv,\n",
    "                                              y_train_cv)\n",
    "        galaxyModel= getVggAngleModel()\n",
    "        \n",
    "        # 调整训练参数\n",
    "        galaxyModel.fit_generator(\n",
    "                gen_flow,\n",
    "                steps_per_epoch=24,\n",
    "                #steps_per_epoch=100,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=([X_holdout,X_angle_hold,\n",
    "                                 X_min1_hold,X_max1_hold,X_std1_hold,X_med1_hold,X_mean1_hold,\n",
    "                                 X_min2_hold,X_max2_hold,X_std2_hold,X_med2_hold,X_mean2_hold], \n",
    "                                 Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        galaxyModel.load_weights(filepath=file_path)\n",
    "        #Getting Training Score\n",
    "        score = galaxyModel.evaluate([X_train_cv,X_angle_cv,\n",
    "                                     X_min1_cv,X_max1_cv,X_std1_cv,X_med1_cv,X_mean1_cv,\n",
    "                                     X_min2_cv,X_max2_cv,X_std2_cv,X_med2_cv,X_mean2_cv], \n",
    "                                     y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        #Getting Test Score\n",
    "        score = galaxyModel.evaluate([X_holdout,X_angle_hold,\n",
    "                                     X_min1_hold,X_max1_hold,X_std1_hold,X_med1_hold,X_mean1_hold,\n",
    "                                     X_min2_hold,X_max2_hold,X_std2_hold,X_med2_hold,X_mean2_hold], \n",
    "                                     Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.\n",
    "        pred_valid=galaxyModel.predict([X_holdout,X_angle_hold,\n",
    "                                       X_min1_hold,X_max1_hold,X_std1_hold,X_med1_hold,X_mean1_hold,\n",
    "                                       X_min2_hold,X_max2_hold,X_std2_hold,X_med2_hold,X_mean2_hold])\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "        temp_test=galaxyModel.predict([X_test, X_test_angle,\n",
    "                                      X_test_min1,X_test_max1,X_test_std1,X_test_med1,X_test_mean1,\n",
    "                                      X_test_min2,X_test_max2,X_test_std2,X_test_med2,X_test_mean2])\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train Scores\n",
    "        temp_train=galaxyModel.predict([X_train, X_angle,\n",
    "                                       X_min1,X_max1,X_std1,X_med1,X_mean1,\n",
    "                                       X_min2,X_max2,X_std2,X_med2,X_mean2])\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation= ',log_loss(target_train, y_train_pred_log))\n",
    "    print(' Test Log Loss Validation= ',log_loss(target_train, y_valid_pred_log))\n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayden/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:169: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 23s - loss: 0.8245 - acc: 0.5715 - val_loss: 0.8851 - val_acc: 0.5308\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 11s - loss: 0.6507 - acc: 0.6284 - val_loss: 0.5567 - val_acc: 0.6804\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 13s - loss: 0.5934 - acc: 0.6726 - val_loss: 0.5506 - val_acc: 0.7009\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 13s - loss: 0.5692 - acc: 0.6847 - val_loss: 0.5436 - val_acc: 0.7047\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 13s - loss: 0.5209 - acc: 0.7258 - val_loss: 0.5705 - val_acc: 0.6224\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 14s - loss: 0.5011 - acc: 0.7325 - val_loss: 0.5219 - val_acc: 0.6785\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 13s - loss: 0.4499 - acc: 0.7596 - val_loss: 0.5604 - val_acc: 0.6112\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 14s - loss: 0.4287 - acc: 0.7864 - val_loss: 0.4464 - val_acc: 0.7121\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 15s - loss: 0.3876 - acc: 0.8121 - val_loss: 0.3680 - val_acc: 0.8262\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 17s - loss: 0.3727 - acc: 0.8209 - val_loss: 0.3682 - val_acc: 0.7888\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 18s - loss: 0.3519 - acc: 0.8211 - val_loss: 0.3129 - val_acc: 0.8486\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 18s - loss: 0.3480 - acc: 0.8319 - val_loss: 0.3026 - val_acc: 0.8654\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 18s - loss: 0.3341 - acc: 0.8529 - val_loss: 0.2890 - val_acc: 0.8617\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 17s - loss: 0.3215 - acc: 0.8465 - val_loss: 0.2935 - val_acc: 0.8617\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 18s - loss: 0.3066 - acc: 0.8657 - val_loss: 0.2671 - val_acc: 0.8766\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 17s - loss: 0.3109 - acc: 0.8676 - val_loss: 0.2671 - val_acc: 0.8766\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 17s - loss: 0.3042 - acc: 0.8632 - val_loss: 0.2785 - val_acc: 0.8617\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 18s - loss: 0.2841 - acc: 0.8649 - val_loss: 0.2585 - val_acc: 0.8822\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 17s - loss: 0.2787 - acc: 0.8711 - val_loss: 0.2642 - val_acc: 0.8860\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 18s - loss: 0.2885 - acc: 0.8777 - val_loss: 0.2535 - val_acc: 0.8822\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 17s - loss: 0.2541 - acc: 0.8805 - val_loss: 0.2542 - val_acc: 0.8841\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 17s - loss: 0.2714 - acc: 0.8768 - val_loss: 0.2561 - val_acc: 0.8916\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 17s - loss: 0.2609 - acc: 0.8825 - val_loss: 0.2616 - val_acc: 0.8879\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 17s - loss: 0.2603 - acc: 0.8702 - val_loss: 0.2587 - val_acc: 0.8879\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 17s - loss: 0.2831 - acc: 0.8768 - val_loss: 0.2584 - val_acc: 0.8916\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 17s - loss: 0.2426 - acc: 0.8968 - val_loss: 0.2537 - val_acc: 0.8860\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 18s - loss: 0.2485 - acc: 0.8861 - val_loss: 0.2523 - val_acc: 0.8860\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 18s - loss: 0.2524 - acc: 0.8977 - val_loss: 0.2458 - val_acc: 0.8822\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 18s - loss: 0.2453 - acc: 0.8868 - val_loss: 0.2388 - val_acc: 0.8879\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 17s - loss: 0.2411 - acc: 0.8927 - val_loss: 0.2593 - val_acc: 0.8935\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 18s - loss: 0.2618 - acc: 0.8891 - val_loss: 0.2324 - val_acc: 0.8953\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 17s - loss: 0.2552 - acc: 0.8794 - val_loss: 0.2380 - val_acc: 0.8953\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 18s - loss: 0.2475 - acc: 0.8853 - val_loss: 0.2285 - val_acc: 0.8879\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 17s - loss: 0.2218 - acc: 0.9031 - val_loss: 0.2311 - val_acc: 0.8972\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 17s - loss: 0.2067 - acc: 0.9094 - val_loss: 0.2402 - val_acc: 0.9009\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 17s - loss: 0.2148 - acc: 0.9012 - val_loss: 0.2485 - val_acc: 0.8953\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 17s - loss: 0.2110 - acc: 0.9134 - val_loss: 0.2383 - val_acc: 0.8991\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 17s - loss: 0.2129 - acc: 0.9049 - val_loss: 0.2371 - val_acc: 0.8935\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 17s - loss: 0.2220 - acc: 0.8990 - val_loss: 0.2371 - val_acc: 0.8972\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 17s - loss: 0.2345 - acc: 0.8882 - val_loss: 0.2299 - val_acc: 0.8972\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 17s - loss: 0.2006 - acc: 0.9093 - val_loss: 0.2339 - val_acc: 0.8897\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 17s - loss: 0.2066 - acc: 0.9143 - val_loss: 0.2363 - val_acc: 0.8953\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 17s - loss: 0.1944 - acc: 0.9187 - val_loss: 0.2373 - val_acc: 0.9047\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 17s - loss: 0.2231 - acc: 0.9096 - val_loss: 0.2357 - val_acc: 0.8991\n",
      "Train loss: 0.183393048811\n",
      "Train accuracy: 0.927970066262\n",
      "Test loss: 0.228466899261\n",
      "Test accuracy: 0.887850469072\n",
      "\n",
      "===================FOLD= 1\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 30s - loss: 0.8734 - acc: 0.5777 - val_loss: 0.7454 - val_acc: 0.6150\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 13s - loss: 0.6497 - acc: 0.6390 - val_loss: 0.6344 - val_acc: 0.6355\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 17s - loss: 0.5980 - acc: 0.6530 - val_loss: 0.6234 - val_acc: 0.6710\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 18s - loss: 0.5593 - acc: 0.6856 - val_loss: 0.5853 - val_acc: 0.6355\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 17s - loss: 0.5215 - acc: 0.7117 - val_loss: 0.5640 - val_acc: 0.6822\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 17s - loss: 0.4819 - acc: 0.7455 - val_loss: 0.5702 - val_acc: 0.6523\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 17s - loss: 0.4721 - acc: 0.7554 - val_loss: 0.5782 - val_acc: 0.6187\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 18s - loss: 0.4097 - acc: 0.7861 - val_loss: 0.4942 - val_acc: 0.7178\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 18s - loss: 0.3837 - acc: 0.8051 - val_loss: 0.4229 - val_acc: 0.7925\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 17s - loss: 0.3787 - acc: 0.8108 - val_loss: 0.4419 - val_acc: 0.7551\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 19s - loss: 0.3799 - acc: 0.8126 - val_loss: 0.3822 - val_acc: 0.8000\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 18s - loss: 0.3455 - acc: 0.8272 - val_loss: 0.3444 - val_acc: 0.8280\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 18s - loss: 0.3114 - acc: 0.8449 - val_loss: 0.3156 - val_acc: 0.8729\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 18s - loss: 0.3246 - acc: 0.8381 - val_loss: 0.2905 - val_acc: 0.8804\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 18s - loss: 0.3268 - acc: 0.8446 - val_loss: 0.2803 - val_acc: 0.8972\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 19s - loss: 0.3144 - acc: 0.8575 - val_loss: 0.2746 - val_acc: 0.8953\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 17s - loss: 0.3142 - acc: 0.8558 - val_loss: 0.2751 - val_acc: 0.9121\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 17s - loss: 0.2983 - acc: 0.8654 - val_loss: 0.2891 - val_acc: 0.8785\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 19s - loss: 0.2962 - acc: 0.8588 - val_loss: 0.2651 - val_acc: 0.8935\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 18s - loss: 0.2863 - acc: 0.8666 - val_loss: 0.2562 - val_acc: 0.8953\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 17s - loss: 0.2964 - acc: 0.8652 - val_loss: 0.2614 - val_acc: 0.9084\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 17s - loss: 0.2917 - acc: 0.8668 - val_loss: 0.2888 - val_acc: 0.8785\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 17s - loss: 0.2559 - acc: 0.8747 - val_loss: 0.2568 - val_acc: 0.9103\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 17s - loss: 0.2571 - acc: 0.8903 - val_loss: 0.2735 - val_acc: 0.8860\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 17s - loss: 0.2776 - acc: 0.8739 - val_loss: 0.2730 - val_acc: 0.8935\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 19s - loss: 0.2659 - acc: 0.8780 - val_loss: 0.2460 - val_acc: 0.9065\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 18s - loss: 0.2683 - acc: 0.8783 - val_loss: 0.2453 - val_acc: 0.9009\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 19s - loss: 0.2539 - acc: 0.8886 - val_loss: 0.2392 - val_acc: 0.9215\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 17s - loss: 0.2478 - acc: 0.8907 - val_loss: 0.2699 - val_acc: 0.8916\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 18s - loss: 0.2418 - acc: 0.8851 - val_loss: 0.2373 - val_acc: 0.9121\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 17s - loss: 0.2330 - acc: 0.9029 - val_loss: 0.2544 - val_acc: 0.9009\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 17s - loss: 0.2463 - acc: 0.8896 - val_loss: 0.2504 - val_acc: 0.9065\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 17s - loss: 0.2311 - acc: 0.9016 - val_loss: 0.2616 - val_acc: 0.8972\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 17s - loss: 0.2412 - acc: 0.8948 - val_loss: 0.2396 - val_acc: 0.9084\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 17s - loss: 0.2233 - acc: 0.9024 - val_loss: 0.2601 - val_acc: 0.8935\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 18s - loss: 0.2512 - acc: 0.8832 - val_loss: 0.2366 - val_acc: 0.9178\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 19s - loss: 0.2329 - acc: 0.8923 - val_loss: 0.2272 - val_acc: 0.9252\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 18s - loss: 0.2233 - acc: 0.9012 - val_loss: 0.2208 - val_acc: 0.9290\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 17s - loss: 0.2379 - acc: 0.8925 - val_loss: 0.2223 - val_acc: 0.9271\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 19s - loss: 0.2236 - acc: 0.8962 - val_loss: 0.2184 - val_acc: 0.9178\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 17s - loss: 0.2070 - acc: 0.9120 - val_loss: 0.2189 - val_acc: 0.9252\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 17s - loss: 0.2012 - acc: 0.9151 - val_loss: 0.2400 - val_acc: 0.9121\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 17s - loss: 0.2135 - acc: 0.9081 - val_loss: 0.2351 - val_acc: 0.9084\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 17s - loss: 0.2185 - acc: 0.9019 - val_loss: 0.2289 - val_acc: 0.9065\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 17s - loss: 0.1910 - acc: 0.9188 - val_loss: 0.2336 - val_acc: 0.9178\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 17s - loss: 0.2128 - acc: 0.8975 - val_loss: 0.2223 - val_acc: 0.9178\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 17s - loss: 0.1981 - acc: 0.9103 - val_loss: 0.2255 - val_acc: 0.9103\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 17s - loss: 0.2122 - acc: 0.9029 - val_loss: 0.2188 - val_acc: 0.9234\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 17s - loss: 0.1896 - acc: 0.9171 - val_loss: 0.2501 - val_acc: 0.8935\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 17s - loss: 0.2028 - acc: 0.9046 - val_loss: 0.2309 - val_acc: 0.9121\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 18s - loss: 0.1919 - acc: 0.9149 - val_loss: 0.2180 - val_acc: 0.9196\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 17s - loss: 0.1947 - acc: 0.9197 - val_loss: 0.2217 - val_acc: 0.9178\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 17s - loss: 0.1668 - acc: 0.9300 - val_loss: 0.2450 - val_acc: 0.8991\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 17s - loss: 0.1855 - acc: 0.9241 - val_loss: 0.2298 - val_acc: 0.9178\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 17s - loss: 0.1932 - acc: 0.9186 - val_loss: 0.2264 - val_acc: 0.9121\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 17s - loss: 0.1716 - acc: 0.9231 - val_loss: 0.2324 - val_acc: 0.9140\n",
      "Epoch 57/100\n",
      "24/24 [==============================] - 17s - loss: 0.1629 - acc: 0.9299 - val_loss: 0.2737 - val_acc: 0.9028\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 17s - loss: 0.1665 - acc: 0.9265 - val_loss: 0.2362 - val_acc: 0.9121\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 17s - loss: 0.1872 - acc: 0.9269 - val_loss: 0.2674 - val_acc: 0.8897\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 17s - loss: 0.1747 - acc: 0.9269 - val_loss: 0.2320 - val_acc: 0.9140\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 17s - loss: 0.1788 - acc: 0.9233 - val_loss: 0.2264 - val_acc: 0.9140\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 17s - loss: 0.1616 - acc: 0.9324 - val_loss: 0.2256 - val_acc: 0.9308\n",
      "Train loss: 0.140916820675\n",
      "Train accuracy: 0.944808231993\n",
      "Test loss: 0.217987421628\n",
      "Test accuracy: 0.919626169561\n",
      "\n",
      "===================FOLD= 2\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 34s - loss: 0.9004 - acc: 0.5612 - val_loss: 0.8724 - val_acc: 0.5262\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 13s - loss: 0.6335 - acc: 0.6422 - val_loss: 0.6365 - val_acc: 0.5693\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 18s - loss: 0.5862 - acc: 0.6767 - val_loss: 0.5860 - val_acc: 0.6442\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 17s - loss: 0.5358 - acc: 0.7211 - val_loss: 0.6587 - val_acc: 0.5412\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 17s - loss: 0.4959 - acc: 0.7344 - val_loss: 0.6383 - val_acc: 0.5506\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 18s - loss: 0.4551 - acc: 0.7668 - val_loss: 0.5311 - val_acc: 0.7041\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 18s - loss: 0.4078 - acc: 0.7944 - val_loss: 0.5147 - val_acc: 0.7341\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 18s - loss: 0.3744 - acc: 0.8042 - val_loss: 0.4971 - val_acc: 0.6835\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 17s - loss: 0.3420 - acc: 0.8260 - val_loss: 0.5059 - val_acc: 0.7041\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 18s - loss: 0.3269 - acc: 0.8397 - val_loss: 0.3912 - val_acc: 0.8052\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 18s - loss: 0.3266 - acc: 0.8464 - val_loss: 0.3798 - val_acc: 0.7959\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 18s - loss: 0.3473 - acc: 0.8302 - val_loss: 0.3644 - val_acc: 0.8296\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 17s - loss: 0.3190 - acc: 0.8507 - val_loss: 0.3810 - val_acc: 0.8090\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 18s - loss: 0.2996 - acc: 0.8648 - val_loss: 0.3532 - val_acc: 0.8390\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 18s - loss: 0.2987 - acc: 0.8614 - val_loss: 0.3530 - val_acc: 0.8296\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 18s - loss: 0.2748 - acc: 0.8829 - val_loss: 0.3384 - val_acc: 0.8502\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 18s - loss: 0.2838 - acc: 0.8752 - val_loss: 0.3346 - val_acc: 0.8446\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 17s - loss: 0.2880 - acc: 0.8748 - val_loss: 0.3390 - val_acc: 0.8408\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 17s - loss: 0.2517 - acc: 0.8874 - val_loss: 0.3350 - val_acc: 0.8464\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 17s - loss: 0.2493 - acc: 0.8924 - val_loss: 0.3394 - val_acc: 0.8539\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 18s - loss: 0.2665 - acc: 0.8845 - val_loss: 0.3291 - val_acc: 0.8502\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 17s - loss: 0.2503 - acc: 0.8948 - val_loss: 0.3558 - val_acc: 0.8390\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 18s - loss: 0.2452 - acc: 0.8876 - val_loss: 0.3251 - val_acc: 0.8596\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 18s - loss: 0.2464 - acc: 0.8984 - val_loss: 0.3185 - val_acc: 0.8558\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 17s - loss: 0.2540 - acc: 0.8948 - val_loss: 0.3216 - val_acc: 0.8558\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 17s - loss: 0.2283 - acc: 0.9035 - val_loss: 0.3196 - val_acc: 0.8633\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 17s - loss: 0.2303 - acc: 0.9046 - val_loss: 0.3320 - val_acc: 0.8558\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 17s - loss: 0.2358 - acc: 0.9108 - val_loss: 0.3215 - val_acc: 0.8483\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 17s - loss: 0.2100 - acc: 0.9111 - val_loss: 0.3338 - val_acc: 0.8464\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 17s - loss: 0.2402 - acc: 0.8965 - val_loss: 0.3209 - val_acc: 0.8539\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 18s - loss: 0.2036 - acc: 0.9151 - val_loss: 0.3161 - val_acc: 0.8539\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 17s - loss: 0.2217 - acc: 0.9129 - val_loss: 0.3227 - val_acc: 0.8614\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 18s - loss: 0.2105 - acc: 0.9175 - val_loss: 0.3041 - val_acc: 0.8577\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 17s - loss: 0.2312 - acc: 0.8993 - val_loss: 0.3064 - val_acc: 0.8558\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 18s - loss: 0.2023 - acc: 0.9152 - val_loss: 0.3009 - val_acc: 0.8652\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 17s - loss: 0.1972 - acc: 0.9249 - val_loss: 0.3154 - val_acc: 0.8708\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 17s - loss: 0.1942 - acc: 0.9209 - val_loss: 0.3135 - val_acc: 0.8633\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 17s - loss: 0.2011 - acc: 0.9135 - val_loss: 0.3047 - val_acc: 0.8558\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 18s - loss: 0.2120 - acc: 0.9161 - val_loss: 0.3006 - val_acc: 0.8539\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 17s - loss: 0.1922 - acc: 0.9257 - val_loss: 0.3032 - val_acc: 0.8633\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 17s - loss: 0.1942 - acc: 0.9249 - val_loss: 0.3025 - val_acc: 0.8577\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 17s - loss: 0.1758 - acc: 0.9301 - val_loss: 0.3094 - val_acc: 0.8652\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 18s - loss: 0.2194 - acc: 0.9121 - val_loss: 0.2889 - val_acc: 0.8633\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 17s - loss: 0.1847 - acc: 0.9239 - val_loss: 0.3018 - val_acc: 0.8577\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 17s - loss: 0.1700 - acc: 0.9340 - val_loss: 0.3166 - val_acc: 0.8727\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 17s - loss: 0.1696 - acc: 0.9367 - val_loss: 0.3142 - val_acc: 0.8727\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 17s - loss: 0.1662 - acc: 0.9366 - val_loss: 0.3078 - val_acc: 0.8689\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 17s - loss: 0.1781 - acc: 0.9226 - val_loss: 0.3082 - val_acc: 0.8745\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 17s - loss: 0.1820 - acc: 0.9292 - val_loss: 0.2977 - val_acc: 0.8633\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 17s - loss: 0.1644 - acc: 0.9344 - val_loss: 0.2992 - val_acc: 0.8783\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 17s - loss: 0.1695 - acc: 0.9290 - val_loss: 0.3073 - val_acc: 0.8689\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 17s - loss: 0.1657 - acc: 0.9356 - val_loss: 0.2957 - val_acc: 0.8764\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 17s - loss: 0.1741 - acc: 0.9309 - val_loss: 0.2908 - val_acc: 0.8783\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 17s - loss: 0.1628 - acc: 0.9327 - val_loss: 0.3081 - val_acc: 0.8689\n",
      "Train loss: 0.14458338979\n",
      "Train accuracy: 0.951401869159\n",
      "Test loss: 0.288881739985\n",
      "Test accuracy: 0.863295880596\n",
      "\n",
      " Train Log Loss Validation=  0.169910147692\n",
      " Test Log Loss Validation=  0.245084731238\n"
     ]
    }
   ],
   "source": [
    "preds=myAngleCV(X_train, X_angle, \n",
    "                X_min1,X_max1,X_std1,X_med1,X_mean1,\n",
    "                X_min2,X_max2,X_std2,X_med2,X_mean2,\n",
    "                X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Submission for each day.\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['is_iceberg']=preds\n",
    "submission.to_csv('subXception.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
